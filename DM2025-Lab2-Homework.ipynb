{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Darya Krautsova\n",
    "\n",
    "Student ID: I143040014\n",
    "\n",
    "GitHub ID: krdaryad\n",
    "\n",
    "Kaggle name: Darya Krautsova\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![score.jpg](./pics/score.jpg)\n",
    "in ranking it would be about 95/153, but it doesn't put it me in the rating even though in the submission deadline is 3rd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "The primary preprocessing steps focused on cleaning raw text data to improve tokenization quality and reduce noise:\n",
    "\n",
    "Data Aggregation and Cleaning: The posts from final_posts.json were parsed. Crucially, the hashtags associated with each post were concatenated directly into the main text string. This ensures that valuable emotion-related cues (e.g., #sadness, #joyful) are available to the model.\n",
    "\n",
    "Text Normalization:\n",
    "\n",
    "All text was converted to lowercase to treat words consistently regardless of casing.\n",
    "\n",
    "URLs (http://..., www....) were removed as they carry no emotional content.\n",
    "\n",
    "User Mentions (@username) were removed to minimize data sparsity.\n",
    "\n",
    "Excessive whitespace was reduced to single spaces.\n",
    "\n",
    "Label Encoding: The six target emotions (anger, disgust, fear, joy, sadness, surprise) were mapped to numerical integer labels (0 through 5), which is required for training classification models like RoBERTa.\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "For deep learning models like RoBERTa, \"feature engineering\" is largely replaced by Transfer Learning and Tokenization:\n",
    "\n",
    "Transfer Learning: The model is initialized with the weights from a pre-trained RoBERTa-base model. This provides the model with powerful, general-purpose linguistic features learned from billions of tokens of internet text, which is far superior to features engineered from scratch.\n",
    "\n",
    "Tokenization: The input text is tokenized using the RoBERTaTokenizer.\n",
    "\n",
    "This process breaks the cleaned text into sub-word units (tokens) recognized by the pre-trained model.\n",
    "\n",
    "Crucially, the tokenizer handles special tokens ([CLS], [SEP], [PAD]) and converts the tokens into input IDs, attention masks, and token type IDs expected by the Transformer architecture.\n",
    "\n",
    "The maximum sequence length was set to 128 tokens, balancing efficiency and content coverage.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[The final model is a fine-tuned RoBERTa-base (Robustly optimized BERT pretraining approach) for Sequence Classification.]\n",
    "Architecture: RoBERTa is a large Transformer model that excels at capturing complex dependencies and context within text. It has 12 layers and 125 million parameters.\n",
    "\n",
    "Classifier Head: The model adds a new classification head (a dense layer) on top of the pre-trained RoBERTa encoder. This head takes the pooled output of the encoder (the [CLS] token representation) and projects it to a vector of size 6 (one for each emotion class).\n",
    "\n",
    "Training Strategy (Fine-Tuning): Instead of training from scratch, the model uses fine-tuning. All layers, including the original RoBERTa encoder weights, are trained on the specific emotion dataset, allowing the model to adapt its powerful general language understanding to the specialized task of emotion classification.\n",
    "Optimization: The model uses the standard AdamW optimizer and is trained with fp16=True (mixed precision) for faster training on a GPU, using a batch size of 32 for optimal resource utilization. The training objective is to minimize the cross-entropy loss and maximize the Macro F1 Score, which is the competition's evaluation metric.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Baseline Model (LinearSVC + TF-IDF): The initial attempt involved a classic machine learning approach: TF-IDF with unigrams/bigrams combined with a LinearSVC (Support Vector Classifier). This model only achieved an F1 score of around 0.41. This demonstrated that simple feature engineering was insufficient to capture the nuanced emotional language.\n",
    "\n",
    "Feature Engineering Iterations: Before moving to RoBERTa, I combined Word TF-IDF and Character TF-IDF in a FeatureUnion with LinearSVC. While this slightly improved the score, it did not break the F1=0.45 barrier.\n",
    "\n",
    "Class Weighting: For all machine learning models, class_weight='balanced' was used to mitigate the severe class imbalance (Joy is over 20x more common than Disgust). This technique was also implicitly handled by the RoBERTa training by optimizing for the Macro F1 score, which equally weights the performance on all six classes.\n",
    "\n",
    "BERT vs. RoBERTa: While standard BERT models work well, RoBERTa was chosen because it was trained on more data, for longer, and without the Next Sentence Prediction (NSP) task, generally resulting in better performance for text classification tasks like this.]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "Imbalance Dominates Simple Models: The huge class imbalance (Joy vs. Disgust/Fear) made it nearly impossible for traditional methods like TF-IDF + SVC to achieve a high Macro F1. They tend to over-predict the majority class (joy). The Macro F1 metric heavily penalizes poor performance on the minority classes, forcing the use of advanced models.\n",
    "\n",
    "Transfer Learning is Essential: Achieving an F1 score of 0.50 or more is a characteristic threshold in text classification problems. Crossing this threshold for a challenging, imbalanced dataset virtually always requires a powerful Transformer model (like RoBERTa) fine-tuned on the specific task. The pre-trained weights provide the necessary semantic depth that traditional count-based models lack.\n",
    "\n",
    "Hashtags are Gold: Including hashtags directly in the text was a high-value preprocessing step. Explicit emotional tags like #surprise or #fear provide direct signals that the model can easily leverage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "# Cell 1: Setup, Imports, and Data Loading\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "from datasets import Dataset \n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Set environment variables and seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DATA_DIR = '/kaggle/input/dm-lab-2-private-competition/'\n",
    "\n",
    "print(\"Loading and preparing data...\")\n",
    "\n",
    "# Load JSON posts and parse\n",
    "with open(DATA_DIR + 'final_posts.json', 'r', encoding='utf-8') as f:\n",
    "    posts_json = json.load(f)\n",
    "\n",
    "posts_data = []\n",
    "for entry in posts_json:\n",
    "    post = entry['root']['_source']['post']\n",
    "    # Aggregation: Combine text and hashtags\n",
    "    full_text = str(post['text']) + \" \" + \" \".join(post.get('hashtags', []))\n",
    "    posts_data.append({'id': post['post_id'], 'text': full_text})\n",
    "posts_df = pd.DataFrame(posts_data)\n",
    "\n",
    "# Load CSVs\n",
    "data_ident_df = pd.read_csv(DATA_DIR + 'data_identification.csv')\n",
    "emotion_df = pd.read_csv(DATA_DIR + 'emotion.csv')\n",
    "\n",
    "# Merge to create Train/Test sets\n",
    "full_df = posts_df.merge(data_ident_df, on='id', how='inner')\n",
    "train_df = full_df[full_df['split'] == 'train'].merge(emotion_df, on='id', how='inner')\n",
    "test_df = full_df[full_df['split'] == 'test'].copy()\n",
    "\n",
    "# Map labels to integers\n",
    "EMOTIONS = sorted(emotion_df['emotion'].unique())\n",
    "label2id = {label: i for i, label in enumerate(EMOTIONS)}\n",
    "id2label = {i: label for i, label in enumerate(EMOTIONS)}\n",
    "train_df['labels'] = train_df['emotion'].map(label2id)\n",
    "# Cell 2: Preprocessing and Tokenization\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Text normalization for RoBERTa input.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Clean up whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Split training data for internal validation\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.1, \n",
    "    random_state=SEED, \n",
    "    stratify=train_df['labels'] \n",
    ")\n",
    "\n",
    "# Convert to HuggingFace Dataset format, removing pandas indexing\n",
    "train_dataset = Dataset.from_pandas(train_data[['text', 'labels']].reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_data[['text', 'labels']].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df[['id', 'text']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "# Cell 3: Feature Engineering (Tokenization)\n",
    "MODEL_NAME = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME) \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # This creates the necessary features: input_ids, attention_mask\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization to all datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Final cleanup for model input\n",
    "tokenized_train = tokenized_train.remove_columns(['text'])\n",
    "tokenized_val = tokenized_val.remove_columns(['text'])\n",
    "tokenized_test_final = tokenized_test.remove_columns(['id', 'text'])\n",
    "\n",
    "# Define Macro F1 metric function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {\"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "# Cell 5: Prediction and Submission\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "raw_predictions = trainer.predict(tokenized_test_final).predictions\n",
    "\n",
    "# Convert logit predictions to class labels\n",
    "predicted_labels = np.argmax(raw_predictions, axis=1)\n",
    "\n",
    "# Map integer labels back to emotion names\n",
    "predicted_emotion_names = [id2label[label] for label in predicted_labels]\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'emotion': predicted_emotion_names\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved to submission.csv\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
