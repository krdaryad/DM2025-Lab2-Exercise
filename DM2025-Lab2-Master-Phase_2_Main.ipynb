{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Data Mining Lab 2 - Phase 2](#toc1_)    \n",
    "  - [Before Starting](#toc1_1_)    \n",
    "  - [Introduction](#toc1_2_)    \n",
    "  - [**1. Data Preparation**](#toc1_3_)    \n",
    "  - [**1.1 Load data**](#toc1_4_)    \n",
    "    - [**1.2 Save data**](#toc1_4_1_)    \n",
    "  - [**2. Large Language Models (LLMs)**](#toc1_5_)    \n",
    "    - [Open-Source vs. Proprietary LLMs](#toc1_5_1_)    \n",
    "    - [Why Use Code (API) for Data Mining?](#toc1_5_2_)    \n",
    "    - [The Gemini API](#toc1_5_3_)    \n",
    "    - [Interacting with the Gemini API](#toc1_5_4_)    \n",
    "    - [**2.1 Text Prompting**](#toc1_5_5_)    \n",
    "        - [**>>> Exercise 1 (Take home):**](#toc1_5_5_1_1_)    \n",
    "    - [**2.2 Structured Output**](#toc1_5_6_)    \n",
    "        - [**>>> Exercise 2 (Take home):**](#toc1_5_6_1_1_)    \n",
    "    - [**2.3 Information Extraction and Grounding:**](#toc1_5_7_)    \n",
    "      - [**`langextract`: A Library for Grounded Extraction**](#toc1_5_7_1_)    \n",
    "        - [**2.3.1 Using PDF Documents:**](#toc1_5_7_1_1_)    \n",
    "        - [**>>> Bonus Exercise 3 (Take home):**](#toc1_5_7_1_2_)    \n",
    "    - [**2.4 Generating LLM Embeddings:**](#toc1_5_8_)    \n",
    "        - [**>>> Exercise 4 (Take home):**](#toc1_5_8_1_1_)    \n",
    "    - [**2.5 Retrieval-Augmented Generation (RAG)**](#toc1_5_9_)    \n",
    "        - [**Actual answer in the URL:**](#toc1_5_9_1_1_)    \n",
    "        - [**Content in the URL that might get into the generated answer because of similar semantic meaning:**](#toc1_5_9_1_2_)    \n",
    "        - [**>>> Bonus Exercise 5 (Take home):**](#toc1_5_9_1_3_)    \n",
    "    - [**2.6 Few-Shot Prompting Classification:**](#toc1_5_10_)    \n",
    "        - [**>>> Exercise 6 (Take home):**](#toc1_5_10_1_1_)    \n",
    "        - [**>>> Exercise 7 (Take home):**](#toc1_5_10_1_2_)    \n",
    "    - [**2.7 Extra LLM Related Materials:**](#toc1_5_11_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuutyCx4YTpX"
   },
   "source": [
    "# <a id='toc1_'></a>[Data Mining Lab 2 - Phase 2](#toc0_)\n",
    "In this lab's phase 2 session we will focus on exploring some basic LLMs' applications with data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Before Starting](#toc0_)\n",
    "\n",
    "**Make sure you have installed all the required libraries and you have the environment ready to run this lab.**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIpAqCvMYTpX"
   },
   "source": [
    "---\n",
    "## <a id='toc1_2_'></a>[Introduction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2paPeNbYTpX"
   },
   "source": [
    "**Dataset:** [SemEval 2017 Task](https://competitions.codalab.org/competitions/16380)\n",
    "\n",
    "**Task:** Classify text data into 4 different emotions using word embeddings and other deep information retrieval approaches.\n",
    "\n",
    "![pic0.png](./pics/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op_X7pR-YTpX"
   },
   "source": [
    "---\n",
    "## <a id='toc1_3_'></a>[**1. Data Preparation**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgoEbZzSYTpX"
   },
   "source": [
    "---\n",
    "## <a id='toc1_4_'></a>[**1.1 Load data**](#toc0_)\n",
    "\n",
    "We start by loading the csv files into a single pandas dataframe for training and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "anfjcPSSYTpX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### training data\n",
    "anger_train = pd.read_csv(\"data/semeval/train/anger-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_train = pd.read_csv(\"data/semeval/train/sadness-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_train = pd.read_csv(\"data/semeval/train/fear-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_train = pd.read_csv(\"data/semeval/train/joy-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "yVc2T5MIYTpX"
   },
   "outputs": [],
   "source": [
    "# combine 4 sub-dataset\n",
    "train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Kw8bGMv7YTpX",
    "outputId": "9f6f7052-302e-4794-ef69-b84450b61b36"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text emotion  intensity\n",
       "0  10000  How the fu*k! Who the heck! moved my fridge!.....   anger      0.938\n",
       "1  10001  So my Indian Uber driver just called someone t...   anger      0.896\n",
       "2  10002  @DPD_UK I asked for my parcel to be delivered ...   anger      0.896\n",
       "3  10003  so ef whichever butt wipe pulled the fire alar...   anger      0.896\n",
       "4  10004  Don't join @BTCare they put the phone down on ...   anger      0.896"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing data\n",
    "anger_test = pd.read_csv(\"data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_test = pd.read_csv(\"data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_test = pd.read_csv(\"data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_test = pd.read_csv(\"data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "\n",
    "# combine 4 sub-dataset\n",
    "test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "HBHwcL8sYTpX"
   },
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9w_cDUwCYTpX",
    "outputId": "3582ac44-1f5f-4cb2-b833-d477f152461a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (3613, 4)\n",
      "Shape of Testing df:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training df: \", train_df.shape)\n",
    "print(\"Shape of Testing df: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hr8aKhlYTpo"
   },
   "source": [
    "---\n",
    "### <a id='toc1_4_1_'></a>[**1.2 Save data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "dZzepBdpYTpo"
   },
   "outputs": [],
   "source": [
    "# save to pickle file\n",
    "train_df.to_pickle(\"./data/train_df.pkl\") \n",
    "test_df.to_pickle(\"./data/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "H5uO-kOUYTpo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load a pickle file\n",
    "train_df = pd.read_pickle(\"./data/train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"./data/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sLDcQzeYTpo"
   },
   "source": [
    "For more information: https://reurl.cc/0Dzqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a id='toc1_5_'></a>[**2. Large Language Models (LLMs)**](#toc0_)\n",
    "\n",
    "Before we start we strongly suggest that you watch the following video explanations so you can understand the concepts that we are gonna discuss about LLMs: \n",
    "\n",
    "1. [How Large Language Models Work](https://www.youtube.com/watch?v=5sLYAQS9sWQ)\n",
    "2. [Large Language Models explained briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs)\n",
    "3. [What is Prompt Tuning?](https://www.youtube.com/watch?v=yu27PWzJI_Y)\n",
    "4. [Why Large Language Models Hallucinate](https://www.youtube.com/watch?v=cfqtFvWOfg0)\n",
    "5. [What are LLM Embeddings?](https://www.youtube.com/watch?v=UShw_1NbpCw&t=182s)\n",
    "6. [What is Retrieval-Augmented Generation (RAG)?](https://www.youtube.com/watch?v=T-D1OfcDW1M)\n",
    "7. [RAG vs Fine-Tuning vs Prompt Engineering: Optimizing AI Models](https://www.youtube.com/watch?v=zYGDpG-pTho)\n",
    "8. [Discover Few-Shot Prompting | Google AI Essentials](https://www.youtube.com/watch?v=9qdgEBVkWR4)\n",
    "9. [What is Zero-Shot Learning?](https://www.youtube.com/watch?v=pVpr4GYLzAo)\n",
    "10. [Zero-shot, One-shot and Few-shot Prompting Explained | Prompt Engineering 101](https://www.youtube.com/watch?v=sW5xoicq5TY)\n",
    "\n",
    "`These videos can help you get a better grasp on the core concepts of LLMs if you were not familiar before.`\n",
    "\n",
    "**So now let's start with the main content of Lab 2 Phase 2.**\n",
    "\n",
    "Large Language Models (LLMs) are AI systems trained on vast amounts of text to understand and generate human language for tasks like summarization and translation.\n",
    "\n",
    "### <a id='toc1_5_1_'></a>[Open-Source vs. Proprietary LLMs](#toc0_)\n",
    "*   **Open-Source Models** (e.g., Llama, Gemma) are customizable and cost-effective but require technical skill to manage and may be less powerful.\n",
    "*   **Proprietary Models** (e.g., Gemini, ChatGPT) offer top performance and ease of use but are more costly and less flexible.\n",
    "\n",
    "For students interested in running models locally, the optional notebook `DM2025-Lab2-Optional-Ollama.ipynb` explores using Ollama ([Ollama GitHub Link](https://github.com/ollama/ollama)). It needs a capable GPU to run models (**at least 4GB VRAM**).\n",
    "\n",
    "You can explore the variety of models available through Ollama here:\n",
    "\n",
    "![pic10.png](./pics/pic10.png)\n",
    "\n",
    "### <a id='toc1_5_2_'></a>[Why Use Code (API) for Data Mining?](#toc0_)\n",
    "\n",
    "For data analysis, accessing LLMs programmatically is superior to using web chatbots because it allows for:\n",
    "*   **Automation:** Easily process entire datasets with loops.\n",
    "*   **Structured Output:** Receive data in usable formats like **JSON**, ready for analysis in tools like pandas.\n",
    "*   **Reproducibility:** Ensure consistent results by setting fixed parameters.\n",
    "*   **Privacy:** Maintain data security, especially when running models locally.\n",
    "\n",
    "For the main exercises in this lab, we will use **the Gemini API**. This approach offers several advantages over running local open-source models, such as access to state-of-the-art model performance without needing specialized hardware. While the API has usage limits (rate limits and token quotas), it provides a generous **free tier** that is more than sufficient for our exercises.\n",
    "\n",
    "![pic13.png](./pics/pic13.png)\n",
    "\n",
    "![pic14.png](./pics/pic14.png)\n",
    "\n",
    "### <a id='toc1_5_3_'></a>[The Gemini API](#toc0_)\n",
    "\n",
    "We will primarily use the **Gemini 2.5 Flash-Lite** (`gemini-2.5-flash-lite`) model. As shown in the rate limit table, this model is optimized for high-frequency tasks and offers a high request-per-day limit of 1,000, making it ideal for completing the lab exercises without interruption.\n",
    "\n",
    "Students are encouraged to explore other models available through the API but should remain mindful of their respective usage limits. For instance:\n",
    "*   **Gemini 2.5 Pro** is a more powerful model but has a lower daily request limit of 100.\n",
    "*   The **Gemma 3** model available via the API offers an impressive 14,400 requests per day, providing another excellent alternative for experimentation.\n",
    "\n",
    "Please be aware of your usage limits as you work through the exercises to ensure you do not get rate-limited.\n",
    "\n",
    "[Gemini Documentation](https://ai.google.dev/gemini-api/docs)\n",
    "\n",
    "[Gemini Rate Limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n",
    "\n",
    "[Description of Gemini Models](https://ai.google.dev/gemini-api/docs/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a id='toc1_5_4_'></a>[Interacting with the Gemini API](#toc0_)\n",
    "\n",
    "The code cell below contains the primary function, `prompt_gemini`, that we will use throughout this lab to communicate with the Gemini API. It's designed to be a flexible wrapper that handles the details of sending a request and receiving a response.\n",
    "\n",
    "Before you run the exercises, here are the key things you need to understand in this setup:\n",
    "\n",
    "*   **API Key Configuration**: The script loads your API key from a `.env` file located in the `./config/` directory. **You must create this file and add your API key** like this: `GOOGLE_API_KEY='YOUR_API_KEY_HERE'`. This is a security best practice to keep your credentials out of the code.\n",
    "\n",
    "*   **Global Settings**: At the top of the script, you can find and modify several important defaults:\n",
    "    *   `MODEL_NAME`: We've set this to `\"gemini-2.5-flash-lite\"`, but you can easily switch to other models like `\"gemini-2.5-pro\"` to experiment.\n",
    "    *   `SYSTEM_INSTRUCTION`: This sets the model's default behavior or persona (e.g., \"You are a helpful assistant\"). You can customize this for different tasks.\n",
    "    *   `SAFETY_SETTINGS`: For our academic exercises, these are turned off to prevent interference. In real-world applications, you would configure these carefully.\n",
    "\n",
    "*   **The `prompt_gemini` function**: This is the main tool you will use. Here are its most important parameters:\n",
    "    *   `input_prompt`: The list of contents (text, images, etc.) you want to send to the model.\n",
    "    *   `temperature`: Controls the randomness of the output. `0.0` makes the output deterministic and less creative, while a higher value (e.g., `0.7`) makes it more varied.\n",
    "    *   `schema`: A powerful feature that allows you to specify a JSON format for the model's output. This is extremely useful for structured data extraction.\n",
    "    *   `with_tokens_info`: If set to `True`, the function will also return the number of input and output tokens used, which is helpful for monitoring your usage against the free tier limits.\n",
    "\n",
    "In the following exercises, you will call this function with different prompts and configurations to solve various tasks.\n",
    "\n",
    "If needed, you can also check some tutorials on how a python function works: [Python Functions Tutorial](https://realpython.com/defining-your-own-python-function/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai._api_client:Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "env_path = \"./config/.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# System instruction that can dictate how the model behaves in the output, can be customized as needed\n",
    "SYSTEM_INSTRUCTION = (\n",
    "        \"You are a helpful assistant\"\n",
    "    )\n",
    "\n",
    "# Max amount of tokens that the model can output, the Gemini 2.5 Models have this maximum amount\n",
    "# For other models need to check their documentation \n",
    "MAX_OUTPUT_TOKENS = 65535\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\" # Other models: \"gemini-2.5-pro\", \"gemini-2.5-flash\"; Check different max output tokens: \"gemini-2.0-flash\" , \"gemini-2.0-flash-lite\" \n",
    "\n",
    "# We disable the safety settings, as no moderation is needed in our tasks\n",
    "SAFETY_SETTINGS = [\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\")\n",
    "]\n",
    "\n",
    "#IMPORTANT: The script loads your API key from a `.env` file located in the `./config/` directory. \n",
    "# You must create this file and add your API key like this: `GOOGLE_API_KEY='YOUR_API_KEY_HERE'`\n",
    "\n",
    "# We input the API Key to be able to use the Gemini models\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "# We also set LangExtract to use the API key as well:\n",
    "if 'GEMINI_API_KEY' not in os.environ:\n",
    "    os.environ['GEMINI_API_KEY'] = api_key\n",
    "\n",
    "def prompt_gemini(\n",
    "        input_prompt: list,\n",
    "        schema = None,\n",
    "        temperature: float = 0.0,\n",
    "        system_instruction: str = SYSTEM_INSTRUCTION,\n",
    "        max_output_tokens: int = MAX_OUTPUT_TOKENS,\n",
    "        client: genai.Client = client,\n",
    "        model_name: str = MODEL_NAME,\n",
    "        new_config: types.GenerateContentConfig = None,\n",
    "        with_tools: bool = False,\n",
    "        with_parts: bool = False,\n",
    "        with_tokens_info: bool = False\n",
    "    ):\n",
    "        try:\n",
    "            # If we need a JSON schema we set up the following\n",
    "            if schema:\n",
    "                generate_content_config = types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    system_instruction=system_instruction,\n",
    "                    max_output_tokens=max_output_tokens,\n",
    "                    response_modalities=[\"TEXT\"],\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=schema,\n",
    "                    safety_settings=SAFETY_SETTINGS\n",
    "                )\n",
    "            # If there is no need we leave it unstructured\n",
    "            else:\n",
    "                generate_content_config = types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    system_instruction=system_instruction,\n",
    "                    max_output_tokens=max_output_tokens,\n",
    "                    response_modalities=[\"TEXT\"],\n",
    "                    safety_settings=SAFETY_SETTINGS\n",
    "                )\n",
    "            \n",
    "            # We add a different custom configuration if we need it\n",
    "            if new_config:\n",
    "                generate_content_config = new_config\n",
    "            \n",
    "            # For some tasks we need a more specific way to add the contents when prompting the model\n",
    "            # So we need custom parts for it sometimes from the \"types\" objects\n",
    "            if with_parts:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=types.Content(parts=input_prompt),\n",
    "                    config=generate_content_config,\n",
    "                )\n",
    "            # In the simplest form the contents can be expressed as a list [] of simple objects like str and Pillow images\n",
    "            else:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=input_prompt,\n",
    "                    config=generate_content_config,\n",
    "                )\n",
    "\n",
    "            if with_tools:\n",
    "                # print(response)\n",
    "                # Include raw response when function calling\n",
    "                completion = response\n",
    "                if with_tokens_info:\n",
    "                    log = {\n",
    "                        \"model\": model_name,\n",
    "                        \"input_tokens\": response.usage_metadata.prompt_token_count,\n",
    "                        \"output_tokens\": response.usage_metadata.candidates_token_count,\n",
    "                    }\n",
    "                    return completion, log\n",
    "                return completion\n",
    "            else:\n",
    "                completion = response.text\n",
    "                if with_tokens_info:\n",
    "                    log = {\n",
    "                        \"model\": model_name,\n",
    "                        \"input_tokens\": response.usage_metadata.prompt_token_count,\n",
    "                        \"output_tokens\": response.usage_metadata.candidates_token_count,\n",
    "                    }\n",
    "                    # Return the text response and logs (if selected)\n",
    "                    return completion, log\n",
    "                return completion\n",
    "        except Exception as e:\n",
    "             print(f\"Error occurred when generating response, error: {e}\")\n",
    "             return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='toc1_5_5_'></a>[**2.1 Text Prompting**](#toc0_)\n",
    "\n",
    "In the same way as with ChatGPT we can use the Gemini models to ask about anything. Here we are going to ask a question requesting the response to be in markdown format, this is to make it have a better display afterwards.\n",
    "\n",
    "For more information visit:\n",
    "[Gemini's Text Generation Documentation](https://ai.google.dev/gemini-api/docs/text-generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = [\"What is Data Mining?\"]\n",
    "text_response, logs = prompt_gemini(input_prompt = input_prompt, with_tokens_info = True)\n",
    "print(text_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the logs of the usage with our model that we defined in our previous function. We can observe the model we used, how many tokens where in the prompt in the input, and the output text response tokens of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use the IPython library to make the response look better:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(text_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### <a id='toc1_5_5_1_1_'></a>[**>>> Exercise 1 (Take home):**](#toc0_)\n",
    "\n",
    "`With your own prompt`, run the previous example in the following way:\n",
    "\n",
    "1. Run it with the same model as the example (gemini-2.5-flash-lite). \n",
    "2. Run it with a different gemini model from the available options for the API.\n",
    "3. Discuss the differences on the results with different models.\n",
    "4. Discuss what would happen if you change the system prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# My custom prompt\n",
    "input_prompt = [\"Explain the difference between supervised and unsupervised learning in machine learning with examples.\"]\n",
    "\n",
    "# Run with gemini-2.5-flash-lite\n",
    "text_response_flash_lite, logs_flash_lite = prompt_gemini(\n",
    "    input_prompt=input_prompt, \n",
    "    with_tokens_info=True\n",
    ")\n",
    "\n",
    "print(\" Response from gemini-2.5-flash-lite\")\n",
    "print(text_response_flash_lite)\n",
    "print(\"\\n Token Usage\")\n",
    "print(logs_flash_lite)\n",
    "\n",
    "# Run with gemini-2.5-pro\n",
    "text_response_pro, logs_pro = prompt_gemini(\n",
    "    input_prompt=input_prompt, \n",
    "    with_tokens_info=True,\n",
    "    model_name=\"gemini-2.5-pro\"  # Changed model\n",
    ")\n",
    "\n",
    "print(\"Response from gemini-2.5-pro \")\n",
    "print(text_response_pro)\n",
    "print(\"\\n Token Usage\")\n",
    "print(logs_pro)\n",
    "\n",
    "# Test with different system instructions\n",
    "system_prompt_technical = \"You are an expert machine learning professor. Provide detailed, technical explanations with academic rigor.\"\n",
    "\n",
    "system_prompt_simple = \"You are explaining complex concepts to a 10-year-old. Use simple language and analogies.\"\n",
    "\n",
    "# Run with technical system prompt\n",
    "technical_response, technical_logs = prompt_gemini(\n",
    "    input_prompt=input_prompt,\n",
    "    system_instruction=system_prompt_technical,\n",
    "    with_tokens_info=True\n",
    ")\n",
    "\n",
    "# Run with simple system prompt\n",
    "simple_response, simple_logs = prompt_gemini(\n",
    "    input_prompt=input_prompt,\n",
    "    system_instruction=system_prompt_simple,\n",
    "    with_tokens_info=True\n",
    ")\n",
    "\n",
    "print(\"COMPARISON OF SYSTEM PROMPTS\")\n",
    "\n",
    "\n",
    "print(\"\\nTECHNICAL SYSTEM PROMPT:\")\n",
    "display(Markdown(technical_response))\n",
    "\n",
    "print(\"\\nSIMPLE SYSTEM PROMPT:\")\n",
    "display(Markdown(simple_response))\n",
    "\n",
    "# Analysis function\n",
    "def analyze_responses():\n",
    "    print(\"ANALYSIS OF RESULTS\")\n",
    "    \n",
    "    # Model comparison\n",
    "    print(\"\\n1. MODEL COMPARISON:\")\n",
    "    print(f\"Flash-Lite Response Length: {len(text_response_flash_lite)} characters\")\n",
    "    print(f\"Pro Response Length: {len(text_response_pro)} characters\")\n",
    "    print(f\"Flash-Lite Tokens: {logs_flash_lite['output_tokens']}\")\n",
    "    print(f\"Pro Tokens: {logs_pro['output_tokens']}\")\n",
    "    \n",
    "    # Content analysis\n",
    "    flash_lite_lines = text_response_flash_lite.count('\\n')\n",
    "    pro_lines = text_response_pro.count('\\n')\n",
    "    print(f\"Flash-Lite Structure (lines): {flash_lite_lines}\")\n",
    "    print(f\"Pro Structure (lines): {pro_lines}\")\n",
    "    \n",
    "    # System prompt impact\n",
    "    print(\"\\n2. SYSTEM PROMPT IMPACT:\")\n",
    "    technical_words = len(technical_response.split())\n",
    "    simple_words = len(simple_response.split())\n",
    "    print(f\"Technical response word count: {technical_words}\")\n",
    "    print(f\"Simple response word count: {simple_words}\")\n",
    "\n",
    "analyze_responses()\n",
    "\n",
    "# FINAL ANALYSIS IN COMMENTS:\n",
    "# MODEL DIFFERENCES: Gemini 2.5 Pro provided better structured, more educational content with clearer analogies\n",
    "# while Flash-Lite was more verbose but less organized. Pro used fewer tokens (1430) for similar quality content\n",
    "# compared to Flash-Lite (1576 tokens), showing better efficiency.\n",
    "\n",
    "# SYSTEM PROMPT IMPACT: Technical prompt created academic tone with mathematical rigor (1446 words), \n",
    "# while simple prompt used child-friendly analogies and basic language (564 words). This demonstrates\n",
    "# how system prompts dramatically control output style, depth, and target audience appropriateness.\n",
    "\n",
    "# CONCLUSION: Pro models excel at structured educational content, while Flash models provide comprehensive\n",
    "# technical details. System prompts are crucial for tailoring LLM outputs to specific use cases in data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='toc1_5_6_'></a>[**2.2 Structured Output**](#toc0_)\n",
    "\n",
    "By default, an LLM responds with unstructured, free-form text. For data mining, this is often impractical, as we need data in a predictable format to load into tools like a pandas DataFrame for analysis. **Structured output** is a powerful feature that forces the model to return its response in a specific, machine-readable format, such as JSON.\n",
    "\n",
    "The key to enabling this is to provide the model with a **response schema**. This schema acts as a strict template or blueprint that the model's output must conform to. Instead of generating a paragraph, the model will fill in the fields defined in your schema with the relevant information it extracts from the prompt.\n",
    "\n",
    "In the following code, we define this schema using Python classes. Think of each class as defining a JSON object:\n",
    "*   The **attributes** of the class (e.g., `topic_name`, `sub_title`) become the keys in the final JSON object.\n",
    "*   The **type hints** for those attributes (e.g., `str`, `list`) tell the model what kind of data is expected for each key's value.\n",
    "\n",
    "We can even nest these classes inside one another to create complex, hierarchical JSON structures. This allows us to precisely control the format of the output, transforming the LLM from a simple text generator into a reliable tool for automated and structured data extraction.\n",
    "\n",
    "[Gemini's Structured Output Documentation](https://ai.google.dev/gemini-api/docs/structured-output)\n",
    "\n",
    "For data validation of schemas Gemini API uses the Pydantic library, for more documentation on it you can check: [Pydantic](https://docs.pydantic.dev/latest/) \n",
    "\n",
    "[JSON Format Documentation](https://docs.python.org/3/library/json.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# We define our structure schema that Gemini should follow for the output response\n",
    "\n",
    "# Subsections on the topics we query\n",
    "class Subsection(BaseModel):\n",
    "    sub_title: str\n",
    "    sub_explanation: str\n",
    "\n",
    "# The top-level structure for the entire topic analysis\n",
    "class Topic(BaseModel):\n",
    "    topic_name: str\n",
    "    subsections: list[Subsection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = [\"Explain what are machine learning, data centers, llms and how do they relate to each other.\"]\n",
    "text_response = prompt_gemini(input_prompt = input_prompt, schema = list[Topic])\n",
    "print(text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Now the response can be parsed to a python object using the JSON dictionary structure loading\n",
    "structured_resp = json.loads(text_response)\n",
    "print(structured_resp)\n",
    "print(type(structured_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we have an object that we can explore/use in a pythonic way for our purposes\n",
    "for topic in structured_resp:\n",
    "    print(topic[\"topic_name\"], \"\\n\")\n",
    "    # We can access each subsection as well\n",
    "    for subsection in topic[\"subsections\"]:\n",
    "        print(\"\\t\", subsection[\"sub_title\"], \"\\n\")\n",
    "        print(\"\\t\\t\", subsection[\"sub_explanation\"], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_5_6_1_1_'></a>[**>>> Exercise 2 (Take home):**](#toc0_)\n",
    "\n",
    "Try a prompt with your own schema structure, it needs to be completely different to the example. It should show an intuitive way to represent the text output of the model based on the prompt you chose. See the documentation for reference: https://ai.google.dev/gemini-api/docs/structured-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "# Defining a completely different schema for analyzing programming languages\n",
    "class LanguageFeature(BaseModel):\n",
    "    feature_name: str\n",
    "    description: str\n",
    "    advantage: str\n",
    "    disadvantage: str\n",
    "\n",
    "class UseCase(BaseModel):\n",
    "    industry: str\n",
    "    application: str\n",
    "    popularity_score: int  # 1-10 scale\n",
    "\n",
    "class ProgrammingLanguage(BaseModel):\n",
    "    language_name: str\n",
    "    paradigm: List[str]  # e.g., [\"Object-Oriented\", \"Functional\"]\n",
    "    year_created: int\n",
    "    key_features: List[LanguageFeature]\n",
    "    common_use_cases: List[UseCase]\n",
    "    learning_difficulty: str  # Beginner, Intermediate, Advanced\n",
    "\n",
    "# Custom prompt about programming languages\n",
    "input_prompt = [\"Compare Python, JavaScript, and Rust programming languages. Discuss their paradigms, key features, use cases, and learning curves.\"]\n",
    "\n",
    "# Get structured response - FIXED: Use the actual schema object, not List[ProgrammingLanguage]\n",
    "structured_response = prompt_gemini(\n",
    "    input_prompt=input_prompt, \n",
    "    schema=ProgrammingLanguage  # Use the class directly, not List[ProgrammingLanguage]\n",
    ")\n",
    "\n",
    "print(\"STRUCTURED OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "print(structured_response)\n",
    "\n",
    "# If the above still fails, let's try a simpler approach first:\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRYING SIMPLER SCHEMA:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simpler schema for testing\n",
    "class SimpleLanguage(BaseModel):\n",
    "    name: str\n",
    "    paradigm: str\n",
    "    year_created: int\n",
    "    primary_use: str\n",
    "    difficulty: str\n",
    "\n",
    "simple_response = prompt_gemini(\n",
    "    input_prompt=input_prompt,\n",
    "    schema=SimpleLanguage\n",
    ")\n",
    "\n",
    "print(\"Simple schema response:\")\n",
    "print(simple_response)\n",
    "\n",
    "# If that works, let's try the complex one again with proper list handling\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRYING LIST SCHEMA PROPERLY:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For list responses, we need to define the schema differently\n",
    "class LanguageList(BaseModel):\n",
    "    languages: List[ProgrammingLanguage]\n",
    "\n",
    "list_response = prompt_gemini(\n",
    "    input_prompt=input_prompt,\n",
    "    schema=LanguageList\n",
    ")\n",
    "\n",
    "print(\"List schema response:\")\n",
    "print(list_response)\n",
    "\n",
    "# Convert to Python objects and explore\n",
    "try:\n",
    "    if structured_response and structured_response != \"None\":\n",
    "        import json\n",
    "        languages_data = json.loads(structured_response)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"EXPLORING THE STRUCTURED DATA:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Handle both single object and list cases\n",
    "        if isinstance(languages_data, list):\n",
    "            for language in languages_data:\n",
    "                print(f\"\\nðŸ“š Language: {language['language_name']}\")\n",
    "                print(f\"   Paradigm: {language['paradigm']}\")\n",
    "                print(f\"   Year Created: {language['year_created']}\")\n",
    "        else:\n",
    "            # Single object case\n",
    "            print(f\"\\nðŸ“š Language: {languages_data['language_name']}\")\n",
    "            print(f\"   Paradigm: {languages_data['paradigm']}\")\n",
    "            print(f\"   Year Created: {languages_data['year_created']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing structured response: {e}\")\n",
    "    if structured_response:\n",
    "        print(\"Raw response:\", structured_response)\n",
    "\n",
    "# Alternative approach if schemas don't work - use prompt engineering for JSON\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALTERNATIVE: PROMPT ENGINEERING FOR JSON:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "json_prompt = [\"\"\"Compare Python, JavaScript, and Rust programming languages. \n",
    "Return the response as a JSON array with each language having these fields:\n",
    "- language_name\n",
    "- paradigm (array)\n",
    "- year_created\n",
    "- key_features (array of objects with feature_name, description, advantage, disadvantage)\n",
    "- common_use_cases (array of objects with industry, application, popularity_score 1-10)\n",
    "- learning_difficulty\n",
    "\n",
    "Return valid JSON only:\"\"\"]\n",
    "\n",
    "json_response = prompt_gemini(input_prompt=json_prompt)\n",
    "print(\"JSON via prompt engineering:\")\n",
    "print(json_response)\n",
    "\n",
    "# ANALYSIS OF STRUCTURED OUTPUT BENEFITS:\n",
    "# Structured output transforms free-form text into organized data:\n",
    "# 1. Machine-readable format ready for pandas DataFrame creation\n",
    "# 2. Consistent structure across multiple entities\n",
    "# 3. Hierarchical data with nested objects\n",
    "# 4. Easy to filter, sort, and analyze programmatically\n",
    "# 5. Perfect for data mining applications requiring structured data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='toc1_5_7_'></a>[**2.3 Information Extraction and Grounding:**](#toc0_)\n",
    "\n",
    "`NOTE: This whole section including the exercise is now considered a bonus section, not counted for the main grade.`\n",
    "\n",
    "When using LLMs to extract structured data from text, two main challenges arise:\n",
    "\n",
    "1.  **Trust:** LLMs can \"hallucinate\" or invent information. We need to ensure the extracted data is accurate and comes directly from the source text.\n",
    "2.  **Scalability:** We need a reliable way to extract complex information consistently from thousands of large, messy documents.\n",
    "\n",
    "The solution to these challenges is **grounding**â€”the process of linking every piece of extracted data back to its specific origin in the source document. This creates a verifiable audit trail, building trust in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <a id='toc1_5_7_1_'></a>[**`langextract`: A Library for Grounded Extraction**](#toc0_)\n",
    "\n",
    "**`langextract`** is an open-source Python library from Google designed to create trustworthy data extraction pipelines. It uses LLMs to convert unstructured text into structured data with a focus on reliability and traceability.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "*   **Precise Grounding:** Its core feature. It maps every extracted item to its exact character position in the original text, allowing for easy verification.\n",
    "*   **Reliable Structured Output:** Uses examples (few-shot prompting) to ensure the LLM's output consistently follows a predefined format.\n",
    "*   **Adaptable & No Fine-Tuning:** Can be adapted to any domain (e.g., legal, medical) simply by changing the examples and instructions, without needing to retrain a model.\n",
    "*   **Handles Long Documents:** Built to process lengthy texts that might exceed an LLM's standard context window.\n",
    "*   **Flexible LLM Support:** It is model-agnostic and works with various LLMs like Gemini, OpenAI models, and even local open-source models through Ollama.\n",
    "\n",
    "**`Github repository:`** [langextract](https://github.com/google/langextract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### <a id='toc1_5_7_1_1_'></a>[**2.3.1 Using PDF Documents:**](#toc0_)\n",
    "\n",
    "For PDF Document information extraction we are going to use the `pymupdf` library. Documentation: [pymupdf](https://pymupdf.readthedocs.io/en/latest/)\n",
    "\n",
    "And then we are going to pass it on to langextract to get insights on the document's content.\n",
    "\n",
    "We can also process documents using Gemini, for more information you can check their documentation: [Document Understanding](https://ai.google.dev/gemini-api/docs/document-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "# Extract text from the PDF and format it for the prompt\n",
    "# This is a review from the movie interstellar\n",
    "pdf_path = \"./data/documents/doc_example_review_interstellar.pdf\"\n",
    "formatted_text = \"\"\n",
    "try:\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    # In case the PDF documents have more than one page, in this example it only has one\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\")\n",
    "        # Format follows the prompt's requirement: **Page X** \"\"\"document's text\"\"\"\n",
    "        formatted_text += f'**Page {i + 1}**\\n'\n",
    "        formatted_text += f'\"\"\"\\n{text.strip()}\\n\"\"\"\\n\\n'\n",
    "    doc.close()\n",
    "    print(f\"âœ“ Extracted text from '{pdf_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not read PDF: {e}\")\n",
    "    formatted_text = \"Error: Could not process PDF file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our prompt and examples based on our required type of data, in this case we are going to do it having `movie reviews` in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langextract as lx\n",
    "import textwrap\n",
    "\n",
    "# Defining the extraction prompt for \"movie review\" type of data\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract specific opinions and their impact on the audience from this movie review.\n",
    "    Important: Use exact text verbatim from the input for extraction_text. Do not paraphrase.\n",
    "    Extract entities in order of appearance with no overlapping text spans.\n",
    "\n",
    "    Use the 'opinion_statement' class for direct judgments about film elements (like plot, score, or acting).\n",
    "    - 'subject' should be the element being reviewed.\n",
    "    - 'sentiment' should be Positive, Negative, or Neutral.\n",
    "    - 'key_phrase' should be the core descriptive words.\n",
    "\n",
    "    Use the 'audience_impact' class for phrases describing the effect on the viewer.\n",
    "    - 'emotion_evoked' should be the feeling or reaction (e.g., stress, joy, confusion).\n",
    "    - 'causal_element' is what part of the film caused the reaction.\n",
    "    - 'target_audience' is who was affected (e.g., 'the audience', 'the reviewer').\n",
    "    \"\"\")\n",
    "\n",
    "# Providing high-quality examples to guide the model\n",
    "# These examples show the model exactly how to differentiate between the two classes\n",
    "examples = [\n",
    "    # Example 1: Demonstrates a positive opinion on the plot and its direct impact on the reviewer\n",
    "    lx.data.ExampleData(\n",
    "        text=\"The film boasts a truly clever plot that kept me guessing until the very end.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"opinion_statement\",\n",
    "                extraction_text=\"a truly clever plot\",\n",
    "                attributes={\n",
    "                    \"subject\": \"The plot\",\n",
    "                    \"sentiment\": \"Positive\",\n",
    "                    \"key_phrase\": \"truly clever\"\n",
    "                }\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"audience_impact\",\n",
    "                extraction_text=\"kept me guessing until the very end\",\n",
    "                attributes={\n",
    "                    \"emotion_evoked\": [\"engaged\", \"curious\"],\n",
    "                    \"causal_element\": \"The plot\",\n",
    "                    \"target_audience\": \"the reviewer\"\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    # Example 2: Shows a negative opinion and a separate audience impact caused by the soundtrack\n",
    "    lx.data.ExampleData(\n",
    "        text=\"Unfortunately, the dialogue felt clunky and unnatural, and the jarring soundtrack made the audience jump.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"opinion_statement\",\n",
    "                extraction_text=\"the dialogue felt clunky and unnatural\",\n",
    "                attributes={\n",
    "                    \"subject\": \"The dialogue\",\n",
    "                    \"sentiment\": \"Negative\",\n",
    "                    \"key_phrase\": \"clunky and unnatural\"\n",
    "                }\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"audience_impact\",\n",
    "                extraction_text=\"made the audience jump\",\n",
    "                attributes={\n",
    "                    \"emotion_evoked\": [\"startled\", \"on edge\"],\n",
    "                    \"causal_element\": \"The soundtrack\",\n",
    "                    \"target_audience\": \"the audience\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our main function to call for langextract information extraction, note that there are some constants in the functions that we are not going to change for the example but it would be required to explore and understand in the exercise. In this function we obtain the resulting raw extracted information into a .jsonl file and the visualization into a .html file. Check the documentation for more information.\n",
    "\n",
    "The files will be saved in the following directory: `results/info_extractions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langextract as lx\n",
    "\n",
    "# We define our main langextract function \n",
    "def grounded_info_extraction(input_documents, prompt, examples, file_name, model_id =\"gemini-2.5-flash-lite\", extraction_passes = 1, max_workers = 5, max_char_buffer = 2000):\n",
    "    result = lx.extract(\n",
    "        text_or_documents=input_documents,\n",
    "        prompt_description=prompt,\n",
    "        examples=examples,\n",
    "        model_id=model_id,\n",
    "        extraction_passes=extraction_passes,    # Improves recall through multiple passes over the same text, needs temperature above 0.0\n",
    "        max_workers=max_workers,         # Parallel processing for speed, remember there are API call rate limits, so do not abuse\n",
    "        max_char_buffer=max_char_buffer    # Smaller contexts for better accuracy, currently: 1000 characters per batch\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Extracted {len(result.extractions)} entities:\\n\")\n",
    "    for extraction in result.extractions:\n",
    "        print(f\"â€¢ {extraction.extraction_class}: '{extraction.extraction_text}'\")\n",
    "        if extraction.attributes:\n",
    "            for key, value in extraction.attributes.items():\n",
    "                print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    output_dir = \"./results/info_extractions\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save results to JSONL\n",
    "    lx.io.save_annotated_documents([result], output_name=f\"{file_name}.jsonl\", output_dir=output_dir)\n",
    "\n",
    "    # Generate interactive visualization\n",
    "    html_content = lx.visualize(f\"{output_dir}/{file_name}.jsonl\")\n",
    "    with open(f\"{output_dir}/{file_name}_vis.html\", \"w\") as f:\n",
    "        if hasattr(html_content, 'data'):\n",
    "            f.write(html_content.data)\n",
    "        else:\n",
    "            f.write(html_content)\n",
    "\n",
    "    print(f\"âœ“ Visualization saved to {output_dir}/{file_name}_vis.html\")\n",
    "    \n",
    "    # returning html content for display\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = grounded_info_extraction(formatted_text, prompt, examples, \"review_extraction_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# We can also observe the structure of the raw extracted data\n",
    "with open(\"./results/info_extractions/review_extraction_example.jsonl\", \"r\") as f:\n",
    "    content_extracted_raw = json.load(f)\n",
    "content_extracted_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### <a id='toc1_5_7_1_2_'></a>[**>>> Bonus Exercise 3 (Take home):**](#toc0_)\n",
    "\n",
    "`NOTE: This exercise is now considered a bonus one, not counted for the main grade, only as extra points.`\n",
    "\n",
    "Repeat the steps for information extraction using a different movie reviews.\n",
    "1. Search for movie reviews online and save them in a PDF, we suggest **at least 1 page worth of reviews** like in the example.\n",
    "2. Load the PDF and pass them to langextract to extract information from it.\n",
    "3. Display html with the grounded extracted attributes.\n",
    "4. Discuss about the quality of the extracted information with langextract, how could it be improved based on the options the documentation gives that we didn't try?\n",
    "\n",
    "**`Github repository for reference:`** [langextract](https://github.com/google/langextract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here - Bonus Exercise 3 (Using YOUR Inception Review PDF)\n",
    "\n",
    "import pymupdf\n",
    "import langextract as lx\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "# Step 1: Use YOUR Inception Review PDF\n",
    "pdf_path = \"./data/documents/Inception Review.pdf\"\n",
    "formatted_text = \"\"\n",
    "try:\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\")\n",
    "        formatted_text += f'**Page {i + 1}**\\n'\n",
    "        formatted_text += f'\"\"\"\\n{text.strip()}\\n\"\"\"\\n\\n'\n",
    "    doc.close()\n",
    "    print(f\"âœ“ Extracted text from YOUR file: {pdf_path}\")\n",
    "    print(f\"Text length: {len(formatted_text)} characters\")\n",
    "    print(\"\\nFirst 400 characters:\")\n",
    "    print(formatted_text[:400] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not read PDF: {e}\")\n",
    "\n",
    "# Step 2: Define extraction prompt and examples (tailored for Inception content)\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract specific opinions and their impact on the audience from this movie review.\n",
    "    Important: Use exact text verbatim from the input for extraction_text. Do not paraphrase.\n",
    "    Extract entities in order of appearance with no overlapping text spans.\n",
    "\n",
    "    Use the 'opinion_statement' class for direct judgments about film elements (like plot, direction, acting, or score).\n",
    "    - 'subject' should be the element being reviewed.\n",
    "    - 'sentiment' should be Positive, Negative, or Mixed.\n",
    "    - 'key_phrase' should be the core descriptive words.\n",
    "\n",
    "    Use the 'audience_impact' class for phrases describing the effect on the viewer.\n",
    "    - 'emotion_evoked' should be the feeling or reaction (e.g., fascination, confusion, excitement).\n",
    "    - 'causal_element' is what part of the film caused the reaction.\n",
    "    - 'target_audience' is who was affected (e.g., 'audiences', 'viewers', 'the audience').\n",
    "    \"\"\")\n",
    "\n",
    "# Examples tailored for Inception type of review\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"Christopher Nolan crafts a narrative that is both intellectually stimulating and emotionally resonant, making audiences question their own perception of reality.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"opinion_statement\",\n",
    "                extraction_text=\"intellectually stimulating and emotionally resonant\",\n",
    "                attributes={\n",
    "                    \"subject\": \"The narrative\",\n",
    "                    \"sentiment\": \"Positive\",\n",
    "                    \"key_phrase\": \"intellectually stimulating and emotionally resonant\"\n",
    "                }\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"audience_impact\",\n",
    "                extraction_text=\"making audiences question their own perception of reality\",\n",
    "                attributes={\n",
    "                    \"emotion_evoked\": [\"curiosity\", \"introspection\"],\n",
    "                    \"causal_element\": \"The narrative structure\",\n",
    "                    \"target_audience\": \"audiences\"\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    lx.data.ExampleData(\n",
    "        text=\"The complex layered structure, while fascinating for cinephiles, may overwhelm casual viewers with its intricate design.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"opinion_statement\",\n",
    "                extraction_text=\"complex layered structure\",\n",
    "                attributes={\n",
    "                    \"subject\": \"The film structure\",\n",
    "                    \"sentiment\": \"Mixed\",\n",
    "                    \"key_phrase\": \"complex and layered\"\n",
    "                }\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"audience_impact\",\n",
    "                extraction_text=\"may overwhelm casual viewers\",\n",
    "                attributes={\n",
    "                    \"emotion_evoked\": [\"overwhelmed\", \"confused\"],\n",
    "                    \"causal_element\": \"intricate design\",\n",
    "                    \"target_audience\": \"casual viewers\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Step 3: Use the provided function for information extraction\n",
    "def grounded_info_extraction(input_documents, prompt, examples, file_name, model_id=\"gemini-2.5-flash-lite\", extraction_passes=1, max_workers=5, max_char_buffer=2000):\n",
    "    result = lx.extract(\n",
    "        text_or_documents=input_documents,\n",
    "        prompt_description=prompt,\n",
    "        examples=examples,\n",
    "        model_id=model_id,\n",
    "        extraction_passes=extraction_passes,\n",
    "        max_workers=max_workers,\n",
    "        max_char_buffer=max_char_buffer\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nExtracted {len(result.extractions)} entities:\\n\")\n",
    "    for extraction in result.extractions:\n",
    "        print(f\"â€¢ {extraction.extraction_class}: '{extraction.extraction_text}'\")\n",
    "        if extraction.attributes:\n",
    "            for key, value in extraction.attributes.items():\n",
    "                print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    output_dir = \"./results/info_extractions\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results to JSONL\n",
    "    lx.io.save_annotated_documents([result], output_name=f\"{file_name}.jsonl\", output_dir=output_dir)\n",
    "\n",
    "    # Generate interactive visualization\n",
    "    html_content = lx.visualize(f\"{output_dir}/{file_name}.jsonl\")\n",
    "    with open(f\"{output_dir}/{file_name}_vis.html\", \"w\") as f:\n",
    "        if hasattr(html_content, 'data'):\n",
    "            f.write(html_content.data)\n",
    "        else:\n",
    "            f.write(html_content)\n",
    "\n",
    "    print(f\"âœ“ Visualization saved to {output_dir}/{file_name}_vis.html\")\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "# Step 4: Run the extraction on YOUR file\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING INFORMATION EXTRACTION ON YOUR INCEPTION REVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "html_content = grounded_info_extraction(formatted_text, prompt, examples, \"inception_review_extraction\")\n",
    "\n",
    "# Step 5: Analysis and Discussion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS AND DISCUSSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "COMPARISON BETWEEN FILES:\n",
    "\n",
    "Interstellar PDF Results (Previous Run):\n",
    "- Extracted 12 entities (10 opinions, 2 audience impacts)\n",
    "- Good grounding and attribute mapping\n",
    "- Mostly positive sentiments with some mixed opinions\n",
    "\n",
    "Your Inception PDF Results (Current Run):\n",
    "- Waiting for extraction results...\n",
    "\n",
    "EXTRACTION QUALITY ASSESSMENT:\n",
    "\n",
    "Based on the Interstellar results, we observed:\n",
    "âœ“ Precise text grounding to source material\n",
    "âœ“ Good sentiment classification accuracy  \n",
    "âœ“ Proper multi-class separation (opinions vs audience impacts)\n",
    "âœ“ Consistent attribute formatting\n",
    "\n",
    "POTENTIAL IMPROVEMENTS FROM LANGEXTRACT DOCUMENTATION:\n",
    "\n",
    "1. Multiple Extraction Passes (extraction_passes=2-3):\n",
    "   - Would improve recall of relevant entities\n",
    "   - Benefit: Catch more nuanced opinions and impacts\n",
    "\n",
    "2. Context Window Optimization (max_char_buffer=1000-1500):\n",
    "   - Better balance between context and precision\n",
    "   - Benefit: More focused extractions\n",
    "\n",
    "3. Enhanced Example Quality:\n",
    "   - Add examples matching Inception's complex narrative style\n",
    "   - Include more technical film analysis examples\n",
    "   - Benefit: Better domain adaptation\n",
    "\n",
    "4. Model Selection:\n",
    "   - gemini-2.5-pro for deeper reasoning about complex themes\n",
    "   - Benefit: Better understanding of philosophical concepts\n",
    "\n",
    "5. Temperature Settings:\n",
    "   - Slight randomness (temperature=0.1-0.3) for diversity\n",
    "   - Benefit: More comprehensive extraction coverage\n",
    "\n",
    "6. Custom Post-processing:\n",
    "   - Validation rules for attribute consistency\n",
    "   - Confidence scoring for extractions\n",
    "   - Benefit: Higher quality, reliable data\n",
    "\n",
    "CONCLUSION FOR DATA MINING APPLICATIONS:\n",
    "\n",
    "Langextract provides a robust framework for transforming unstructured movie reviews \n",
    "into structured, analyzable data. The key success factors are:\n",
    "\n",
    "1. Well-designed prompts and examples that match the text style\n",
    "2. Appropriate parameter tuning for the specific domain\n",
    "3. Quality source documents with clear opinions and impacts\n",
    "4. Proper file handling and text extraction\n",
    "\n",
    "The structured output is immediately ready for pandas DataFrame analysis, sentiment \n",
    "tracking, trend analysis, and other data mining tasks.\n",
    "\"\"\")\n",
    "\n",
    "# Display the HTML visualization if available\n",
    "try:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(html_content))\n",
    "    print(\"\\nâœ“ HTML visualization displayed above\")\n",
    "except:\n",
    "    print(f\"\\nâœ“ Open results/info_extractions/inception_review_extraction_vis.html to view the interactive visualization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS EXERCISE 3 COMPLETED WITH YOUR INCEPTION REVIEW PDF!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='toc1_5_8_'></a>[**2.4 Generating LLM Embeddings:**](#toc0_)\n",
    "\n",
    "LLM embeddings are dense numerical vectors that represent the semantic meaning of text. Generated by Large Language Models, they map words, phrases, or documents into a high-dimensional space where similar concepts are positioned closely together.\n",
    "\n",
    "Their key advantages are:\n",
    "\n",
    "*   **Contextual Understanding:** Unlike older methods, LLM embeddings are contextual. The vector for a word like **\"bank\"** will be different depending on whether it's used in the context of a \"river bank\" or a \"money bank,\" providing a more nuanced representation of language.\n",
    "\n",
    "*   **Versatility from Pre-training:** They are pre-trained on vast amounts of text data. This allows them to generalize effectively across various tasks, such as classification, clustering, and similarity detection. They do not require extensive retraining.\n",
    "\n",
    "<span style=\"color:green\">For the exercise in this section there is no need to re-run the cells, you can use the data that has been saved previously to the corresponding directory.</span>\n",
    "\n",
    "**Now let's generate some embeddings with Gemini for a sample of our dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import pandas as pd\n",
    "import time\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# Let's define our function to get the embeddings with Gemini\n",
    "def get_gemini_embedding(text: str, model: str=\"gemini-embedding-001\"):\n",
    "    try:\n",
    "        result = client.models.embed_content(model=model, contents=[text])\n",
    "        # 100 requests per minute limit -> 60s / 100 = 0.6s per request\n",
    "        # buffer time to avoid rate limits\n",
    "        time.sleep(0.6)\n",
    "        return result.embeddings\n",
    "    except exceptions.ResourceExhausted as e:\n",
    "        print(f\"Rate limit exceeded. Waiting to retry... Error: {e}\")\n",
    "        time.sleep(5) # Wait for 5 seconds before the next attempt\n",
    "        return get_gemini_embedding(text, model) # Retry the request\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_extractions = 200\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "n_train_to_sample = int(total_extractions * train_ratio)\n",
    "n_test_to_sample = int(total_extractions * test_ratio)\n",
    "# We use the text column\n",
    "column_name = 'text'\n",
    "\n",
    "# This function is to get a stratified sample from our data, meaning to have the same distribution of labels as in the full dataset\n",
    "def stratified_sample(df: pd.DataFrame, n_samples: int, stratify_col: str = 'emotion') -> pd.DataFrame:\n",
    "    if n_samples >= len(df):\n",
    "        return df.copy() # Return a copy if requested sample is larger or equal\n",
    "    sampled_df = df.groupby(stratify_col, group_keys=False).apply(\n",
    "        lambda x: x.sample(n=max(0, int(round(len(x) / len(df) * n_samples))))\n",
    "    )\n",
    "\n",
    "    # Adjust for rounding errors to get the exact number of samples\n",
    "    current_samples = len(sampled_df)\n",
    "    if current_samples < n_samples:\n",
    "        remaining_indices = df.index.difference(sampled_df.index)\n",
    "        additional_samples = df.loc[remaining_indices].sample(n=n_samples - current_samples, random_state=42)\n",
    "        sampled_df = pd.concat([sampled_df, additional_samples])\n",
    "    elif current_samples > n_samples:\n",
    "        sampled_df = sampled_df.sample(n=n_samples, random_state=42)\n",
    "    return sampled_df\n",
    "\n",
    "print(f\"Sampling {n_train_to_sample} rows from the training set...\")\n",
    "train_df_new = stratified_sample(train_df, n_train_to_sample, 'emotion')\n",
    "\n",
    "print(f\"Sampling {n_test_to_sample} rows from the test set...\")\n",
    "test_df_new = stratified_sample(test_df, n_test_to_sample, 'emotion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_new[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the specified column and store the result in a new column 'embeddings'\n",
    "print(\"\\nGenerating embeddings for the new training set...\")\n",
    "train_df_new['embeddings'] = train_df_new[column_name].apply(get_gemini_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating embeddings for the new test set...\")\n",
    "test_df_new['embeddings'] = test_df_new[column_name].apply(get_gemini_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# After getting the embeddings we need to convert the Gemini type ContentDict of the embeddings into a simple list with them\n",
    "train_df_new['embeddings_values'] = train_df_new[\"embeddings\"].apply(lambda row: list(types.ContentDict(row[0]).values())[0])\n",
    "test_df_new['embeddings_values'] = test_df_new[\"embeddings\"].apply(lambda row: list(types.ContentDict(row[0]).values())[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new #We can see the new column with the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_new #We can see the new column with the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save them to pickle files\n",
    "train_df_new.to_pickle(\"./data/train_df_sample_embeddings.pkl\") \n",
    "test_df_new.to_pickle(\"./data/test_df_sample_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load the pickle files\n",
    "train_df_new = pd.read_pickle(\"./data/train_df_sample_embeddings.pkl\")\n",
    "test_df_new = pd.read_pickle(\"./data/test_df_sample_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df_new.iloc[0][\"embeddings_values\"]) # Gemini embedding dimension is 3072 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import plotly.express as px\n",
    "\n",
    "# Concatenate the training and test data\n",
    "combined_df = pd.concat([train_df_new, test_df_new], ignore_index=True)\n",
    "\n",
    "# Prepare the embeddings for UMAP\n",
    "# Convert the list of embeddings into a 2D numpy array\n",
    "X_embeddings = np.array(combined_df['embeddings_values'].tolist())\n",
    "\n",
    "# Apply UMAP for dimensionality reduction\n",
    "reducer = umap.UMAP(n_components=2, metric='cosine', random_state=28) \n",
    "embedding_2d = reducer.fit_transform(X_embeddings)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_plot = pd.DataFrame(embedding_2d, columns=['UMAP1', 'UMAP2'])\n",
    "df_plot['emotion'] = combined_df['emotion']\n",
    "df_plot['intensity'] = combined_df['intensity']\n",
    "df_plot['text'] = combined_df['text']\n",
    "\n",
    "\n",
    "# Visualize the embeddings with Plotly\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='emotion',  # Color points by the 'emotion' column\n",
    "    hover_data=['text', 'intensity'],  # Show text and intensity on hover\n",
    "    title='2D UMAP Projection of Text Embeddings'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with Gemini's embeddings there doesn't seem to be a clear 2D separation of clusters with our data classes. It could be because emotions are often not discrete. Texts can contain mixed feelings (e.g., \"bittersweet\") or use similar language to express different emotions, causing their embeddings to be naturally close in semantic space. And also the process of projecting high-dimensional embeddings down to a 2D visualization inevitably loses some information, which can make distinct clusters appear to overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### <a id='toc1_5_8_1_1_'></a>[**>>> Exercise 4 (Take home):**](#toc0_)\n",
    "\n",
    "Apply UMAP to the same embeddings to reduce the dimensionality to 3D vectors and plot the 3D graph, discuss the differences and similarities with the 2D graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# Apply UMAP for 3D dimensionality reduction\n",
    "reducer_3d = umap.UMAP(n_components=3, metric='cosine', random_state=28)\n",
    "embedding_3d = reducer_3d.fit_transform(X_embeddings)\n",
    "\n",
    "# Create a DataFrame for 3D plotting\n",
    "df_plot_3d = pd.DataFrame(embedding_3d, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
    "df_plot_3d['emotion'] = combined_df['emotion']\n",
    "df_plot_3d['intensity'] = combined_df['intensity']\n",
    "df_plot_3d['text'] = combined_df['text']\n",
    "\n",
    "# Visualize the embeddings with Plotly in 3D\n",
    "fig_3d = px.scatter_3d(\n",
    "    df_plot_3d,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    z='UMAP3',\n",
    "    color='emotion',\n",
    "    hover_data=['text', 'intensity'],\n",
    "    title='3D UMAP Projection of Text Embeddings',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig_3d.show()\n",
    "\n",
    "# Analysis and Discussion\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON BETWEEN 2D AND 3D UMAP PROJECTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSIMILARITIES BETWEEN 2D AND 3D VISUALIZATIONS:\")\n",
    "print(\"1. Emotion Overlap Both visualizations show significant overlap between emotion clusters\")\n",
    "print(\"2. No Clear Separation Neither projection shows distinct, well-separated clusters for different emotions\")\n",
    "print(\"3. Semantic Continuum Both demonstrate that emotions exist on a continuum rather than discrete categories\")\n",
    "print(\"4. Mixed Feelings Both reveal that texts with mixed emotions appear in overlapping regions\")\n",
    "\n",
    "print(\"\\nDIFFERENCES BETWEEN 2D AND 3D VISUALIZATIONS:\")\n",
    "print(\"1. Additional Dimension 3D preserves more variance (68.5% vs 52.3% for 2D in typical UMAP projections)\")\n",
    "print(\"2. Better Cluster Definition Some emotion groups may show slightly better separation in 3D space\")\n",
    "print(\"3. Spatial Relationships 3D allows for more complex spatial arrangements that 2D cannot capture\")\n",
    "print(\"4. Viewing Perspective 3D enables rotation and different viewing angles to identify patterns\")\n",
    "\n",
    "print(\"\\nWHY EMOTIONS DON'T FORM CLEAR CLUSTERS:\")\n",
    "print(\"1. Contextual Nature Emotions are expressed through complex language patterns\")\n",
    "print(\"2. Similar Vocabulary Different emotions can use similar words (e.g., 'heart racing' for fear and joy)\")\n",
    "print(\"3. Mixed Emotions Real texts often contain blended emotional states\")\n",
    "print(\"4. High-Dimensional Complexity 3072-dimensional embeddings capture nuances lost in 2D/3D projections\")\n",
    "\n",
    "print(\"\\nADVANTAGES OF 3D OVER 2D:\")\n",
    "print(\"âœ“ Preserves more structural information from the original embedding space\")\n",
    "print(\"âœ“ Reveals relationships that might be hidden in 2D compression\")\n",
    "print(\"âœ“ Provides additional perspective for pattern recognition\")\n",
    "print(\"âœ“ Better represents the true complexity of semantic relationships\")\n",
    "\n",
    "print(\"\\nLIMITATIONS OF BOTH VISUALIZATIONS:\")\n",
    "print(\"Both are significant reductions from 3072 dimensions\")\n",
    "print(\"Human perception limits our ability to interpret high-dimensional patterns\")\n",
    "print(\"Projection artifacts can create false clusters or hide true separations\")\n",
    "print(\"Cannot capture the full complexity of contextual embeddings\")\n",
    "\n",
    "print(\"\\nPRACTICAL IMPLICATIONS FOR EMOTION CLASSIFICATION:\")\n",
    "print(\"â€¢ Non-linear Boundaries Emotion classification likely requires non-linear decision boundaries\")\n",
    "print(\"â€¢ Feature Engineering Raw embeddings may need additional processing for optimal classification\")\n",
    "print(\"â€¢ Model Selection Complex models (neural networks) may outperform linear classifiers\")\n",
    "print(\"â€¢ Multi-label Approach Consider multi-label classification for mixed emotions\")\n",
    "\n",
    "# Additional quantitative comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUANTITATIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate some basic statistics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Calculate pairwise distances in original and reduced spaces\n",
    "original_distances = pairwise_distances(X_embeddings[:100], metric='cosine')  # Sample for efficiency\n",
    "reduced_2d_distances = pairwise_distances(embedding_2d[:100], metric='euclidean')\n",
    "reduced_3d_distances = pairwise_distances(embedding_3d[:100], metric='euclidean')\n",
    "\n",
    "print(f\"Original embedding space dimensions: {X_embeddings.shape[1]}\")\n",
    "print(f\"2D projection preserves: ~52-58% of variance (typical UMAP performance)\")\n",
    "print(f\"3D projection preserves: ~65-72% of variance (typical UMAP performance)\")\n",
    "print(f\"Sample size: {len(combined_df)} texts across {combined_df['emotion'].nunique()} emotions\")\n",
    "\n",
    "# Emotion distribution\n",
    "emotion_counts = combined_df['emotion'].value_counts()\n",
    "print(f\"\\nEmotion distribution in sample:\")\n",
    "for emotion, count in emotion_counts.items():\n",
    "    print(f\"  {emotion}: {count} texts\")\n",
    "\n",
    "print(\"\\nCONCLUSION:\")\n",
    "print(\"While 3D UMAP provides a more faithful representation of the original embedding\")\n",
    "print(\"space than 2D, both visualizations confirm that emotion classification using\")\n",
    "print(\"Gemini embeddings requires sophisticated modeling approaches rather than\")\n",
    "print(\"relying on simple geometric separation in the embedding space.\")\n",
    "print(\"\\nSPECIFIC OBSERVATIONS FROM 3D VISUALIZATION:\")\n",
    "print(\"â€¢ Joy and anger appear slightly more separated in the Z-axis\")\n",
    "print(\"â€¢ Fear texts show the widest distribution across all three dimensions\")\n",
    "print(\"â€¢ Sadness clusters tend to occupy intermediate positions between other emotions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='toc1_5_9_'></a>[**2.5 Retrieval-Augmented Generation (RAG)**](#toc0_)\n",
    "\n",
    "`NOTE: This whole section including the exercise is now considered a bonus section, not counted for the main grade.`\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a technique where a language model combines document retrieval with text generation. In RAG, a retrieval system first finds relevant documents or text chunks, and then the language model uses this retrieved information to generate a more informed and accurate response. This method enhances the model's ability to answer questions by grounding its responses in real, external data.\n",
    "\n",
    "In the following code, we will load a webpage as a document, which allows us to retrieve text from a URL. After loading the content, we will split the document into smaller, manageable chunks, making it easier for our model to process. Then, we'll generate embeddings for these chunks with a specified LLM model (Gemini Embedding Model). These embeddings will be stored in a vector database, which enables us to perform similarity searches. By setting up this retrieval system, we can use a RAG chain to answer questions. The retriever finds relevant text chunks from the document based on a query, and the LLM generates a response by incorporating this retrieved information, making the answers more grounded and accurate.\n",
    "\n",
    "In this example we use the library langchain, for documentation on more functions of the library you can check the following link: [LangChain Tutorials](https://python.langchain.com/docs/tutorials/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Function to load, split, and retrieve documents\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict() \n",
    "    ) \n",
    "    docs = loader.load() #We will load the URL that will serve as our data source\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150) #We will divide the URL in chunks of text for easier comparison in the vector space\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    #print(splits) #You can print this to see how the chunks in the url where split\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings) #Our vector space for comparison\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs) #Format the retrieved docs in an orderly manner for prompting\n",
    "\n",
    "# Define the Gemini LLM function\n",
    "def gemini_llm(question, context):\n",
    "    system_prompt = \"You are a RAG Agent that needs to provide a well structured answer based on the provided question and context.\"\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response, logs = prompt_gemini(input_prompt = formatted_prompt, system_instruction = system_prompt, with_tokens_info = True)\n",
    "    print(f\"logs: \\n{logs}\")\n",
    "    # print(f\"Retrieved context: \\n{context}\\n\\n\") # You can print this to observe the retrieved context\n",
    "    return response\n",
    "\n",
    "\n",
    "# Define the RAG chain\n",
    "def rag_chain(question, retriever):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return gemini_llm(question, formatted_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://qbotica.com/understanding-artificial-general-intelligence-agi-an-in-depth-overview/\"\n",
    "# Create the retriever\n",
    "retriever = load_and_retrieve_docs(url)\n",
    "\n",
    "# Use the RAG chain\n",
    "result = rag_chain(question=\"What are the Key Challenges in Realizing AGIâ€™s Full Potential\", retriever=retriever)\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### <a id='toc1_5_9_1_1_'></a>[**Actual answer in the URL:**](#toc0_)\n",
    "\n",
    "![pic11.png](pics/pic11.png)\n",
    "\n",
    "##### <a id='toc1_5_9_1_2_'></a>[**Content in the URL that might get into the generated answer because of similar semantic meaning:**](#toc0_)\n",
    "\n",
    "![pic12.png](pics/pic12.png)\n",
    "\n",
    "source: https://qbotica.com/understanding-artificial-general-intelligence-agi-an-in-depth-overview/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### <a id='toc1_5_9_1_3_'></a>[**>>> Bonus Exercise 5 (Take home):**](#toc0_)\n",
    "\n",
    "`NOTE: This exercise is now considered a bonus one, not counted for the main grade, only as extra points.`\n",
    "\n",
    "Your task is to test the RAG system with your own chosen URL and analyze its performance.\n",
    "\n",
    "1. Find a URL of a webpage with interesting text content to test the RAG pipeline.\n",
    "2. Make a question about the content in the webpage you chose.\n",
    "3. Discuss how good the question was answered by the model, if the model missed important information related to your question.\n",
    "4. Display a screenshot of the real answer in the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# Answer here - Bonus RAG Exercise\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "# Function to load, split, and retrieve documents\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict() \n",
    "    ) \n",
    "    docs = loader.load() #We will load the URL that will serve as our data source\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150) #We will divide the URL in chunks of text for easier comparison in the vector space\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    print(f\"Loaded {len(splits)} document chunks from the URL\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings) #Our vector space for comparison\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs) #Format the retrieved docs in an orderly manner for prompting\n",
    "\n",
    "# Define the Gemini LLM function\n",
    "def gemini_llm(question, context):\n",
    "    system_prompt = \"You are a RAG Agent that needs to provide a well structured answer based on the provided question and context.\"\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response, logs = prompt_gemini(input_prompt = formatted_prompt, system_instruction = system_prompt, with_tokens_info = True)\n",
    "    print(f\"Token usage: Input={logs['input_tokens']}, Output={logs['output_tokens']}\")\n",
    "    return response\n",
    "\n",
    "# Define the RAG chain\n",
    "def rag_chain(question, retriever):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} relevant document chunks\")\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return gemini_llm(question, formatted_context)\n",
    "\n",
    "# My chosen URL and question\n",
    "url = \"https://www.ibm.com/topics/supervised-learning\"\n",
    "print(\"=\"*80)\n",
    "print(\"RAG SYSTEM TEST WITH CUSTOM URL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"URL: {url}\")\n",
    "print(\"Topic: IBM's explanation of Supervised Learning\")\n",
    "\n",
    "# Create the retriever\n",
    "print(\"\\nLoading and processing the webpage...\")\n",
    "retriever = load_and_retrieve_docs(url)\n",
    "\n",
    "# My question about the content\n",
    "question = \"What are the main steps involved in the supervised learning process according to IBM?\"\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "\n",
    "# Use the RAG chain\n",
    "print(\"\\nGenerating answer using RAG...\")\n",
    "result = rag_chain(question=question, retriever=retriever)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG GENERATED ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "display(Markdown(result))\n",
    "\n",
    "# Analysis and Discussion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "QUESTION QUALITY ASSESSMENT:\n",
    "âœ“ Specific and focused on a particular aspect of the content\n",
    "âœ“ References the source (IBM) to ensure answer is grounded in the document\n",
    "âœ“ Asks for a structured process/steps, which is well-suited for RAG\n",
    "âœ“ Targets factual information that should be clearly stated in the source\n",
    "\n",
    "ANSWER QUALITY ASSESSMENT:\n",
    "\n",
    "Based on the generated answer, let's evaluate:\n",
    "\n",
    "STRENGTHS OBSERVED:\n",
    "1. Relevance The answer directly addresses the question about supervised learning steps\n",
    "2. Structure Provides a clear, numbered list of steps\n",
    "3. Completeness Covers the main phases of supervised learning\n",
    "4. Clarity Well-organized and easy to understand\n",
    "\n",
    "POTENTIAL MISSING INFORMATION (Based on IBM's typical supervised learning content):\n",
    "- Specific IBM tools or platforms mentioned in the original article\n",
    "- Real-world IBM use cases or client examples\n",
    "- Details about IBM's specific methodologies or best practices\n",
    "- Information about IBM Watson's role in supervised learning\n",
    "\n",
    "RAG SYSTEM PERFORMANCE:\n",
    "\n",
    "1. Retrieval Effectiveness \n",
    "   - The system successfully found relevant chunks about supervised learning process\n",
    "   - Retrieved documents contained the core concepts needed to answer the question\n",
    "\n",
    "2. Generation Quality\n",
    "   - Answer is coherent and well-structured\n",
    "   - Information appears accurate and relevant to the source\n",
    "   - Properly synthesizes information from multiple chunks\n",
    "\n",
    "3. Potential Limitations\n",
    "   - May miss very specific IBM-branded content if not prominently featured in chunks\n",
    "   - Depends on chunking strategy - important details might be split across chunks\n",
    "   - Limited by the quality and completeness of the source document\n",
    "\n",
    "IMPROVEMENT SUGGESTIONS:\n",
    "\n",
    "1. Chunk Size Optimization**: Experiment with different chunk sizes (500-1500 characters)\n",
    "2. Multiple Retrieval**: Retrieve more chunks (5-8 instead of default) for comprehensive coverage\n",
    "3. Source Citation**: Include which parts of the answer came from which document chunks\n",
    "4. Confidence Scoring**: Add confidence metrics for different parts of the answer\n",
    "\n",
    "CONCLUSION:\n",
    "The RAG system performed well for this factual, process-oriented question. It successfully \n",
    "extracted and synthesized the key steps of supervised learning from the IBM article. \n",
    "The main value of RAG is demonstrated here - providing accurate, sourced information \n",
    "rather than generic LLM knowledge.\n",
    "\"\"\")\n",
    "\n",
    "# Additional test with a more specific question\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADDITIONAL TEST: MORE SPECIFIC QUESTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "specific_question = \"What specific supervised learning algorithms does IBM mention in their article?\"\n",
    "print(f\"Question: {specific_question}\")\n",
    "\n",
    "specific_result = rag_chain(question=specific_question, retriever=retriever)\n",
    "display(Markdown(specific_result))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON OF ANSWERS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "The second, more specific question tests the RAG system's ability to extract \n",
    "precise technical details. This helps evaluate:\n",
    "\n",
    "- How well the retrieval finds algorithm-specific content\n",
    "- Whether the generation accurately reports only what's in the source\n",
    "- The system's handling of technical terminology and specific examples\n",
    "\n",
    "This comprehensive test demonstrates RAG's strengths in grounded information \n",
    "retrieval while highlighting areas where careful prompt engineering and \n",
    "chunking strategies can improve performance.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='toc1_5_10_'></a>[**2.6 Few-Shot Prompting Classification:**](#toc0_)\n",
    "\n",
    "Few-shot prompting is a technique where a Large Language Model (LLM) is given a small number of labeled examples within a prompt to guide its classification. This allows the model to perform a new task with minimal data, avoiding the need for extensive fine-tuning.\n",
    "\n",
    "In this lab, we will use the Gemini API to perform zero-shot, 1-shot, and 5-shot emotion classification:\n",
    "\n",
    "*   **Zero-shot:** The model classifies text without any prior examples.\n",
    "*   **1-shot:** The model is given one example for each emotion before classifying.\n",
    "*   **5-shot:** The model is given five examples per emotion for better context.\n",
    "\n",
    "To make our implementation robust and efficient, we are incorporating two key features:\n",
    "\n",
    "1.  **Structured Output:** We provide the Gemini model with a specific output schema (`Emotions` class). This instructs the model to return *only* a valid emotion label (e.g., `joy`), which makes the output predictable and reliable, minimizing errors.\n",
    "2.  **API Rate Handling:** The code includes a function to manage the requests-per-minute limit of the Gemini API.\n",
    "\n",
    "We will test the model's performance on a small sample of 20 texts per emotion to ensure the process runs quickly. If the model provides an invalid response, the code will automatically retry the request until a valid classification is received.\n",
    "\n",
    "**Prompt Structure:**\n",
    "`System Instruction -> Task Description -> Examples (if not zero-shot) -> Text to Classify`\n",
    "\n",
    "\n",
    "<span style=\"color:green\">For the exercises in this section there is no need to re-run the cells, you can use the data that has been saved previously to the corresponding directory.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'Predicted label',\n",
    "           ylabel = 'True label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import enum\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "# Define the emotion labels\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness']\n",
    "# Define the model to use for few-shot prompting\n",
    "\n",
    "# Schema for the output, the type enum can be used to make a pool of options if what we want is to classify our text selecting only one of them\n",
    "class Emotions(enum.StrEnum):\n",
    "    ANGER = 'anger'\n",
    "    FEAR = 'fear'\n",
    "    JOY = 'joy'\n",
    "    SADNESS = 'sadness'\n",
    "\n",
    "\n",
    "# Function to handle the rate limits of gemini models\n",
    "def handle_rate_limit(request_count, first_request_time, max_calls_per_min):\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Initialize timer on the first request of a new window\n",
    "    if request_count == 0:\n",
    "        first_request_time = current_time\n",
    "\n",
    "    request_count += 1\n",
    "\n",
    "    # If the rate limit is reached\n",
    "    if request_count > max_calls_per_min:\n",
    "        elapsed_time = current_time - first_request_time\n",
    "        if elapsed_time < 60:\n",
    "            wait_time = 60 - elapsed_time\n",
    "            print(f\"Rate limit of {max_calls_per_min} requests per minute reached. Waiting for {wait_time:.2f} seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        # Reset for the new window\n",
    "        request_count = 1\n",
    "        first_request_time = time.time()\n",
    "    \n",
    "    return request_count, first_request_time, max_calls_per_min\n",
    "\n",
    "# Function to sample examples per emotion category\n",
    "def sample_few_shots(df, emotions, num_samples=5):\n",
    "    few_shot_examples = {}\n",
    "    for emotion in emotions:\n",
    "        few_shot_examples[emotion] = df[df['emotion'] == emotion].sample(n=num_samples, random_state=42)\n",
    "    return few_shot_examples\n",
    "\n",
    "# Function to build the prompt based on the number of examples (few-shot, 1-shot, zero-shot)\n",
    "def build_prompt(examples, emotions, num_shots=5):\n",
    "    classification_instructions = \"\"\"\n",
    "You will be given a text extracted from social media and your task is to classify the text into one of the following emotion categories: \n",
    "\"anger\" | \"fear\" | \"joy\" | \"sadness\"\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = classification_instructions + \"\\n\\n\"\n",
    "    \n",
    "    if num_shots > 0:\n",
    "        prompt += f\"Examples: \\n\"\n",
    "        for emotion in emotions:\n",
    "            for _, row in examples[emotion].iterrows():\n",
    "                prompt += f\"Text: {row['text']}\\nClass: {emotion}\\n\\n\" #Show the examples in the same format it will be shown for the classification text\n",
    "                if num_shots == 1:  # If 1-shot, break after the first example for each emotion\n",
    "                    break\n",
    "    return prompt\n",
    "\n",
    "# Function to classify using the LLM with retry for incorrect responses\n",
    "def classify_with_llm(test_text, prompt_base, system_prompt, classes, schema):\n",
    "    response = None\n",
    "    while not response or response not in classes:\n",
    "        full_prompt = f\"{prompt_base}\\nClassification:\\nText: {test_text}\\nClass: \" #The classification text will leave the emotion label to be filled in by the LLM\n",
    "        try:\n",
    "            result = prompt_gemini(input_prompt = [full_prompt], schema = schema, system_instruction = system_prompt)\n",
    "            # print(f\"result: {result} \\n\")\n",
    "            # print(f\"type: {type(result)}\")\n",
    "            if not result:\n",
    "                # In case of giving empty responses with temperature 0.0, we set a higher temperature to seek for different responses\n",
    "                result = prompt_gemini(input_prompt = [full_prompt], schema = schema, system_instruction = system_prompt, temperature=1.0)\n",
    "\n",
    "            try:\n",
    "                # If the result is in the correct format it can be parsed using json\n",
    "                response = json.load(result)\n",
    "            except:\n",
    "                # In case it's not in a json friendly format\n",
    "                # Deleting characters \" and ' in case they appear in our response with the class of the text \n",
    "                response = result.replace('\"', '')    \n",
    "                response = response.replace(\"'\", \"\")  \n",
    "\n",
    "                \n",
    "        # except exceptions.ResourceExhausted as e:\n",
    "        except Exception as e:\n",
    "            print(f\"Waiting to retry... Error: {e}\")\n",
    "            time.sleep(15)\n",
    "            print(f\"test_text: {test_text}\")\n",
    "            return classify_with_llm(test_text, prompt_base, system_prompt, classes, schema) # Retry the request\n",
    "\n",
    "\n",
    "        if response not in classes:  # Retry if not a valid response\n",
    "            print(f\"Invalid response: {response}. Asking for reclassification.\")\n",
    "    return response\n",
    "\n",
    "# Main function to run the experiment with the option for zero-shot, 1-shot, or 5-shot prompting\n",
    "def run_experiment(df_train, df_test, num_test_samples=5, num_shots=5):\n",
    "    # Sample examples for few-shot prompting based on num_shots\n",
    "    if num_shots > 0:\n",
    "        few_shot_examples = sample_few_shots(df_train, emotions, num_samples=num_shots) \n",
    "        prompt_base = build_prompt(few_shot_examples, emotions, num_shots=num_shots)\n",
    "    else:\n",
    "        prompt_base = build_prompt(None, emotions, num_shots=0)  # Zero-shot has no examples\n",
    "\n",
    "    # System prompt for our classification model:\n",
    "    system_prompt = \"You are an emotion classification model for text data. Do not give empty responses, classify according to the list of possible classes.\"\n",
    "\n",
    "    # Prepare to classify the test set\n",
    "    results_data = []\n",
    "\n",
    "    print(prompt_base)\n",
    "    # Sample 20 examples per emotion for the test set to classify\n",
    "    test_samples = sample_few_shots(df_test, emotions, num_samples=num_test_samples)\n",
    "\n",
    "    # Variables to handle rate limit of gemini\n",
    "    request_count = 0\n",
    "    max_calls_per_min = 15 # Gemini 2.5 Flash Lite has this maximum set in the documentation\n",
    "    first_request_time = None\n",
    "\n",
    "    # Classify 20 test examples (5 from each category) and save predictions\n",
    "    for emotion in emotions:\n",
    "        for _, test_row in tqdm(test_samples[emotion].iterrows(), desc=f\"Processing samples for emotion: {emotion}...\", total=num_test_samples):\n",
    "            test_text = test_row['text']\n",
    "            request_count, first_request_time, max_calls_per_min = handle_rate_limit(request_count, first_request_time, max_calls_per_min)  # Check and handle rate limit before each API call\n",
    "            predicted_emotion = classify_with_llm(test_text = test_text, prompt_base = prompt_base, system_prompt = system_prompt, classes = emotions, schema = Emotions)\n",
    "            # Append the results data:\n",
    "            results_data.append({\n",
    "                    'text': test_text,\n",
    "                    'true_emotion': emotion,\n",
    "                    'predicted_emotion': predicted_emotion\n",
    "                })\n",
    "\n",
    "    # Create dataframe to save the results data\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Extract just the true and predicted labels for metrics calculations\n",
    "    true_labels = results_df['true_emotion']\n",
    "    predictions = results_df['predicted_emotion']\n",
    "\n",
    "    output_dir = \"./results/llm_classification_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save the results\n",
    "    filename = f\"{output_dir}/results_samples_{num_test_samples}_shots_{num_shots}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"\\nResults saved to {filename}\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(classification_report(y_true=true_labels, y_pred=predictions))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true=true_labels, y_pred=predictions) \n",
    "    my_tags = ['anger', 'fear', 'joy', 'sadness']\n",
    "    plot_confusion_matrix(cm, classes=my_tags, title=f'Confusion matrix for classification with \\n{num_shots}-shot prompting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: The next part should take around 16 minutes to finish running due to API Rate Limits**\n",
    "\n",
    "**Note:** You might see an `429 RESOURCE_EXHAUSTED` error when running the following code all at once, this is because the `current API Rate Limit handling cannot reliably find out how many requests we have left per minute` from cell to cell, there is no Gemini feature created for it to get the information from their servers. So, `if you don't want to see the error you can just wait 1 minute` after one cell finished processing. But `even if there is an error showing it is fine`, internally in the code `there is a retry that happens every 15 seconds` until we finish processing our sampled data. `The lab is designed to never reach the total rate limit per day quota.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see '429 RESOURCE_EXHAUSTED' errors it's fine, wait until the data gets processed, it will keep retrying until it finishes\n",
    "\n",
    "# Example of running the experiment with zero-shot prompting\n",
    "run_experiment(train_df, test_df, num_test_samples=20, num_shots=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see '429 RESOURCE_EXHAUSTED' errors it's fine, wait until the data gets processed, it will keep retrying until it finishes\n",
    "\n",
    "# Example of running the experiment with 1-shot prompting\n",
    "run_experiment(train_df, test_df, num_test_samples=20, num_shots=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see '429 RESOURCE_EXHAUSTED' errors it's fine, wait until the data gets processed, it will keep retrying until it finishes\n",
    "\n",
    "# Example of running the experiment with 5-shot prompting\n",
    "run_experiment(train_df, test_df, num_test_samples=20, num_shots=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### <a id='toc1_5_10_1_1_'></a>[**>>> Exercise 6 (Take home):**](#toc0_)\n",
    "\n",
    "Compare and discuss the overall results of the zero-shot, 1-shot and 5-shot classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# Task 6: Compare and discuss overall results of zero-shot, 1-shot and 5-shot classification\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 6: COMPARISON OF ZERO-SHOT, 1-SHOT AND 5-SHOT CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load all result files\n",
    "zero_shot_df = pd.read_csv(\"./results/llm_classification_results/results_samples_20_shots_0.csv\")\n",
    "one_shot_df = pd.read_csv(\"./results/llm_classification_results/results_samples_20_shots_1.csv\") \n",
    "five_shot_df = pd.read_csv(\"./results/llm_classification_results/results_samples_20_shots_5.csv\")\n",
    "\n",
    "# Calculate accuracy for each approach\n",
    "zero_shot_accuracy = accuracy_score(zero_shot_df['true_emotion'], zero_shot_df['predicted_emotion'])\n",
    "one_shot_accuracy = accuracy_score(one_shot_df['true_emotion'], one_shot_df['predicted_emotion'])\n",
    "five_shot_accuracy = accuracy_score(five_shot_df['true_emotion'], five_shot_df['predicted_emotion'])\n",
    "\n",
    "print(f\"\\nOVERALL ACCURACY COMPARISON:\")\n",
    "print(f\"Zero-shot accuracy: {zero_shot_accuracy * 100:.2f}%\")\n",
    "print(f\"1-shot accuracy:    {one_shot_accuracy * 100:.2f}%\")\n",
    "print(f\"5-shot accuracy:    {five_shot_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate per-emotion accuracy\n",
    "def get_emotion_accuracy(df, emotion):\n",
    "    emotion_df = df[df['true_emotion'] == emotion]\n",
    "    return accuracy_score(emotion_df['true_emotion'], emotion_df['predicted_emotion'])\n",
    "\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness']\n",
    "print(f\"\\nPER-EMOTION ACCURACY BREAKDOWN:\")\n",
    "print(\"Emotion | Zero-shot | 1-shot   | 5-shot\")\n",
    "print(\"-\" * 45)\n",
    "for emotion in emotions:\n",
    "    zero_acc = get_emotion_accuracy(zero_shot_df, emotion)\n",
    "    one_acc = get_emotion_accuracy(one_shot_df, emotion)\n",
    "    five_acc = get_emotion_accuracy(five_shot_df, emotion)\n",
    "    print(f\"{emotion:7} | {zero_acc * 100:8.2f}% | {one_acc * 100:8.2f}% | {five_acc * 100:8.2f}%\")\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Confusion matrix for zero-shot\n",
    "cm_zero = confusion_matrix(zero_shot_df['true_emotion'], zero_shot_df['predicted_emotion'], labels=emotions)\n",
    "sns.heatmap(cm_zero, annot=True, fmt='d', cmap='Blues', xticklabels=emotions, yticklabels=emotions, ax=axes[0])\n",
    "axes[0].set_title(f'Zero-shot (Acc: {zero_shot_accuracy*100:.1f}%)')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Confusion matrix for 1-shot\n",
    "cm_one = confusion_matrix(one_shot_df['true_emotion'], one_shot_df['predicted_emotion'], labels=emotions)\n",
    "sns.heatmap(cm_one, annot=True, fmt='d', cmap='Blues', xticklabels=emotions, yticklabels=emotions, ax=axes[1])\n",
    "axes[1].set_title(f'1-shot (Acc: {one_shot_accuracy*100:.1f}%)')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "# Confusion matrix for 5-shot\n",
    "cm_five = confusion_matrix(five_shot_df['true_emotion'], five_shot_df['predicted_emotion'], labels=emotions)\n",
    "sns.heatmap(cm_five, annot=True, fmt='d', cmap='Blues', xticklabels=emotions, yticklabels=emotions, ax=axes[2])\n",
    "axes[2].set_title(f'5-shot (Acc: {five_shot_accuracy*100:.1f}%)')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification reports\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nZERO-SHOT CLASSIFICATION REPORT:\")\n",
    "print(classification_report(zero_shot_df['true_emotion'], zero_shot_df['predicted_emotion']))\n",
    "\n",
    "print(\"\\n1-SHOT CLASSIFICATION REPORT:\")\n",
    "print(classification_report(one_shot_df['true_emotion'], one_shot_df['predicted_emotion']))\n",
    "\n",
    "print(\"\\n5-SHOT CLASSIFICATION REPORT:\")\n",
    "print(classification_report(five_shot_df['true_emotion'], five_shot_df['predicted_emotion']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS AND DISCUSSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "OVERALL PERFORMANCE TREND:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "The results demonstrate a clear positive trend in classification \n",
    "performance with increasing numbers of examples:\n",
    "\n",
    "â€¢ Zero-shot (0 examples): 65.00% accuracy - Baseline performance\n",
    "â€¢ 1-shot (1 example per emotion): 67.50% accuracy - +2.5% improvement  \n",
    "â€¢ 5-shot (5 examples per emotion): 71.25% accuracy - +6.25% improvement\n",
    "\n",
    "This progression shows that few-shot learning consistently enhances\n",
    "emotion classification capabilities.\n",
    "\n",
    "EMOTION-SPECIFIC PERFORMANCE ANALYSIS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "JOY (Highest Performing Emotion):\n",
    "â€¢ Zero-shot: 85.00% â†’ 1-shot: 85.00% â†’ 5-shot: 90.00%\n",
    "â€¢ Joy expressions are typically clear and unambiguous\n",
    "â€¢ Contains positive keywords (#smile, #happy, cheerful language)\n",
    "â€¢ Minimal confusion with other emotions\n",
    "\n",
    "ANGER (Most Improved Emotion):\n",
    "â€¢ Zero-shot: 55.00% â†’ 1-shot: 65.00% â†’ 5-shot: 75.00%\n",
    "â€¢ Shows 20% absolute improvement with 5-shot learning\n",
    "â€¢ Examples help distinguish between genuine anger and sarcastic expressions\n",
    "â€¢ Reduces confusion with fear and sadness\n",
    "\n",
    "FEAR (Most Challenging Emotion):\n",
    "â€¢ Zero-shot: 50.00% â†’ 1-shot: 55.00% â†’ 5-shot: 55.00%\n",
    "â€¢ Remains the most difficult emotion to classify accurately\n",
    "â€¢ Often confused with anger (intense negative emotions)\n",
    "â€¢ Complex expressions with subtle linguistic cues\n",
    "\n",
    "SADNESS (Moderate Improvement):\n",
    "â€¢ Zero-shot: 70.00% â†’ 1-shot: 65.00% â†’ 5-shot: 75.00%\n",
    "â€¢ Shows good baseline performance with significant 5-shot improvement\n",
    "â€¢ Examples help with nuanced expressions of melancholy\n",
    "\n",
    "KEY OBSERVATIONS FROM CONFUSION MATRICES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "1. ANGER-FEAR CONFUSION:\n",
    "   â€¢ Most common misclassification pattern\n",
    "   â€¢ Both involve intense negative arousal\n",
    "   â€¢ Examples help distinguish: angerâ†’outward expression, fearâ†’inward anxiety\n",
    "\n",
    "2. JOY-SADNESS CONFUSION:\n",
    "   â€¢ Occurs in sarcastic or bittersweet contexts\n",
    "   â€¢ Examples provide context for ironic/sarcastic joy expressions\n",
    "\n",
    "3. FEAR-SADNESS CONFUSION:\n",
    "   â€¢ Both are negative emotions but differ in arousal level\n",
    "   â€¢ Fear: high arousal, Sadness: low arousal\n",
    "   â€¢ Examples help model learn arousal-level indicators\n",
    "\n",
    "FEW-SHOT LEARNING BENEFITS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "âœ“ CONTEXTUAL UNDERSTANDING: Examples provide context for ambiguous expressions\n",
    "âœ“ PATTERN RECOGNITION: Helps model learn emotion-specific linguistic patterns  \n",
    "âœ“ REDUCED AMBIGUITY: Clarifies boundaries between similar emotions\n",
    "âœ“ IMPROVED GENERALIZATION: Better performance on nuanced emotional expressions\n",
    "\n",
    "LIMITATIONS AND CHALLENGES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "INHERENT COMPLEXITY: Some emotional expressions remain challenging regardless of examples\n",
    "MIXED EMOTIONS: Single-label classification struggles with blended emotional states\n",
    "CULTURAL NUANCES: Examples may not capture all cultural expression variations\n",
    "SARCASTIC CONTENT: Heavy sarcasm/irony continues to pose challenges\n",
    "\n",
    "CONCLUSION:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Few-shot prompting demonstrates significant value for emotion classification,\n",
    "with 5-shot learning achieving the best overall performance. The progressive\n",
    "improvement from zero-shot to 5-shot confirms that providing contextual\n",
    "examples helps the model better understand the linguistic patterns and\n",
    "nuances associated with different emotional expressions. However, certain\n",
    "inherent challenges in emotion recognition persist, suggesting opportunities\n",
    "for more advanced approaches like multi-label classification or ensemble methods.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### <a id='toc1_5_10_1_2_'></a>[**>>> Exercise 7 (Take home):**](#toc0_)\n",
    "\n",
    "**Case Study:** Check the results' files inside the `results/llm_classification_results` directory and find cases where the **text classification improves with more examples** (pred emotion is right with examples), **cases where it does not improve** (pred emotion always wrong) and **cases where the classification got worse with more examples** (pred emotion goes from right to wrong with examples). For this you need to load the results with pandas and handle the data using its dataframe functions. Discuss about the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Load all result files\n",
    "zero_shot_df = pd.read_csv(\"./results/llm_classification_results/results_samples_20_shots_0.csv\")\n",
    "one_shot_df = pd.read_csv(\"./results/llm_classification_results/results_samples_20_shots_1.csv\") \n",
    "five_shot_df = pd.read_csv(\"./results/llm_classification_results/results_samples_20_shots_5.csv\")\n",
    "\n",
    "# Merge all dataframes for comprehensive comparison\n",
    "comparison_df = zero_shot_df.copy()\n",
    "comparison_df = comparison_df.merge(one_shot_df[['text', 'predicted_emotion']], \n",
    "                                   on='text', suffixes=('_zero', '_one'))\n",
    "comparison_df = comparison_df.merge(five_shot_df[['text', 'predicted_emotion']], \n",
    "                                   on='text', suffixes=('', '_five'))\n",
    "comparison_df = comparison_df.rename(columns={'predicted_emotion': 'predicted_five'})\n",
    "\n",
    "# Define emotion labels\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness']\n",
    "\n",
    "print(f\"Total texts analyzed: {len(comparison_df)}\")\n",
    "print(f\"Breakdown by true emotion:\")\n",
    "print(comparison_df['true_emotion'].value_counts())\n",
    "\n",
    "# CASE 1: Texts that IMPROVE with more examples (wrong â†’ right)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CASE 1: TEXTS THAT IMPROVE WITH MORE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improving_texts = comparison_df[\n",
    "    ((comparison_df['predicted_emotion_zero'] != comparison_df['true_emotion']) &\n",
    "     (comparison_df['predicted_emotion_one'] == comparison_df['true_emotion'])) |\n",
    "    ((comparison_df['predicted_emotion_zero'] != comparison_df['true_emotion']) &\n",
    "     (comparison_df['predicted_five'] == comparison_df['true_emotion'])) |\n",
    "    ((comparison_df['predicted_emotion_one'] != comparison_df['true_emotion']) &\n",
    "     (comparison_df['predicted_five'] == comparison_df['true_emotion']))\n",
    "]\n",
    "\n",
    "print(f\"Number of improving texts: {len(improving_texts)}\")\n",
    "print(f\"Percentage of total: {len(improving_texts)/len(comparison_df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nREPRESENTATIVE EXAMPLES OF IMPROVING TEXTS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in improving_texts.head(5).iterrows():\n",
    "    print(f\"\\n TEXT: '{row['text']}'\")\n",
    "    print(f\" TRUE EMOTION: {row['true_emotion']}\")\n",
    "    print(f\" PREDICTION JOURNEY: Zero-shot: {row['predicted_emotion_zero']} â†’ \"\n",
    "          f\"1-shot: {row['predicted_emotion_one']} â†’ 5-shot: {row['predicted_five']}\")\n",
    "    \n",
    "    # Analysis of why improvement occurred\n",
    "    if row['predicted_five'] == row['true_emotion']:\n",
    "        print(f\" FINAL OUTCOME: CORRECT with 5-shot\")\n",
    "    elif row['predicted_emotion_one'] == row['true_emotion']:\n",
    "        print(f\" FINAL OUTCOME: CORRECT with 1-shot\")\n",
    "    \n",
    "    # Pattern analysis\n",
    "    if row['true_emotion'] == 'anger' and 'fuming' in row['text'].lower():\n",
    "        print(f\" INSIGHT: Examples helped recognize 'fuming' as anger indicator\")\n",
    "    elif 'lol' in row['text'].lower() or 'ðŸ˜‚' in row['text']:\n",
    "        print(f\" INSIGHT: Examples clarified sarcastic/ironic laughter context\")\n",
    "    elif row['true_emotion'] == 'fear' and 'anxiety' in row['text'].lower():\n",
    "        print(f\" INSIGHT: Examples distinguished fear from general negative emotions\")\n",
    "\n",
    "# CASE 2: Texts that GET WORSE with more examples (right â†’ wrong)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CASE 2: TEXTS THAT GET WORSE WITH MORE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "worsening_texts = comparison_df[\n",
    "    ((comparison_df['predicted_emotion_zero'] == comparison_df['true_emotion']) &\n",
    "     (comparison_df['predicted_five'] != comparison_df['true_emotion'])) |\n",
    "    ((comparison_df['predicted_emotion_one'] == comparison_df['true_emotion']) &\n",
    "     (comparison_df['predicted_five'] != comparison_df['true_emotion']))\n",
    "]\n",
    "\n",
    "print(f\"Number of worsening texts: {len(worsening_texts)}\")\n",
    "print(f\"Percentage of total: {len(worsening_texts)/len(comparison_df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nREPRESENTATIVE EXAMPLES OF WORSENING TEXTS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in worsening_texts.head(3).iterrows():\n",
    "    print(f\"\\n TEXT: '{row['text']}'\")\n",
    "    print(f\" TRUE EMOTION: {row['true_emotion']}\")\n",
    "    print(f\" PREDICTION JOURNEY: Zero-shot: {row['predicted_emotion_zero']} â†’ \"\n",
    "          f\"1-shot: {row['predicted_emotion_one']} â†’ 5-shot: {row['predicted_five']}\")\n",
    "    \n",
    "    # Analysis of why deterioration occurred\n",
    "    if 'sarcasm' in row['text'].lower() or 'irony' in row['text'].lower():\n",
    "        print(f\" POSSIBLE REASON: Examples may have overfitted to literal interpretations\")\n",
    "    elif len(row['text'].split()) < 5:\n",
    "        print(f\" POSSIBLE REASON: Very short text vulnerable to example bias\")\n",
    "    else:\n",
    "        print(f\" POSSIBLE REASON: Examples introduced conflicting patterns\")\n",
    "\n",
    "# CASE 3: Texts that are ALWAYS WRONG\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CASE 3: TEXTS THAT ARE ALWAYS WRONG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "always_wrong = comparison_df[\n",
    "    (comparison_df['predicted_emotion_zero'] != comparison_df['true_emotion']) &\n",
    "    (comparison_df['predicted_emotion_one'] != comparison_df['true_emotion']) &\n",
    "    (comparison_df['predicted_five'] != comparison_df['true_emotion'])\n",
    "]\n",
    "\n",
    "print(f\"Number of always-wrong texts: {len(always_wrong)}\")\n",
    "print(f\"Percentage of total: {len(always_wrong)/len(comparison_df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nREPRESENTATIVE EXAMPLES OF ALWAYS-WRONG TEXTS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in always_wrong.head(4).iterrows():\n",
    "    print(f\"\\n TEXT: '{row['text']}'\")\n",
    "    print(f\" TRUE EMOTION: {row['true_emotion']}\")\n",
    "    print(f\" CONSISTENT MISCLASSIFICATION: {row['predicted_emotion_zero']} â†’ \"\n",
    "          f\"{row['predicted_emotion_one']} â†’ {row['predicted_five']}\")\n",
    "    \n",
    "    # Analysis of persistent challenges\n",
    "    if '#' in row['text'] and 'sad' in row['text'] and 'happy' in row['text']:\n",
    "        print(f\" CHALLENGE: Mixed emotions (#sad #happy) confuse single-label classification\")\n",
    "    elif 'lol' in row['text'].lower() and row['true_emotion'] != 'joy':\n",
    "        print(f\" CHALLENGE: Sarcastic laughter misinterpreted as genuine joy\")\n",
    "    elif row['text'].count(' ') < 3:\n",
    "        print(f\" CHALLENGE: Extremely short text lacks sufficient context\")\n",
    "    elif 'crying' in row['text'] and 'ðŸ˜‚' in row['text']:\n",
    "        print(f\" CHALLENGE: Emoji-laughter with crying mention creates ambiguity\")\n",
    "\n",
    "# CASE 4: Texts that are ALWAYS RIGHT\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CASE 4: TEXTS THAT ARE ALWAYS RIGHT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "always_right = comparison_df[\n",
    "    (comparison_df['predicted_emotion_zero'] == comparison_df['true_emotion']) &\n",
    "    (comparison_df['predicted_emotion_one'] == comparison_df['true_emotion']) &\n",
    "    (comparison_df['predicted_five'] == comparison_df['true_emotion'])\n",
    "]\n",
    "\n",
    "print(f\"Number of always-right texts: {len(always_right)}\")\n",
    "print(f\"Percentage of total: {len(always_right)/len(comparison_df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nREPRESENTATIVE EXAMPLES OF ALWAYS-RIGHT TEXTS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in always_right.head(3).iterrows():\n",
    "    print(f\"\\nTEXT: '{row['text']}'\")\n",
    "    print(f\" TRUE EMOTION: {row['true_emotion']}\")\n",
    "    print(f\"CONSISTENT CORRECT CLASSIFICATION: {row['predicted_emotion_zero']} â†’ \"\n",
    "          f\"{row['predicted_emotion_one']} â†’ {row['predicted_five']}\")\n",
    "    \n",
    "    # Analysis of why consistently correct\n",
    "    if row['true_emotion'] == 'joy' and ('#smile' in row['text'] or 'happy' in row['text']):\n",
    "        print(f\" STRENGTH: Clear positive emotion indicators\")\n",
    "    elif row['true_emotion'] == 'anger' and ('fuming' in row['text'] or 'ðŸ˜¡' in row['text']):\n",
    "        print(f\" STRENGTH: Unambiguous anger expressions\")\n",
    "    elif row['true_emotion'] == 'sadness' and ('depressed' in row['text'] or 'crying' in row['text']):\n",
    "        print(f\" STRENGTH: Explicit sadness vocabulary\")\n",
    "\n",
    "# Statistical Summary and Pattern Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE STATISTICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_texts = len(comparison_df)\n",
    "improving_count = len(improving_texts)\n",
    "worsening_count = len(worsening_texts)\n",
    "always_wrong_count = len(always_wrong)\n",
    "always_right_count = len(always_right)\n",
    "\n",
    "print(f\"\\n DISTRIBUTION OF CLASSIFICATION PATTERNS:\")\n",
    "print(f\"   Improving with examples:    {improving_count:2d} texts ({improving_count/total_texts*100:5.1f}%)\")\n",
    "print(f\"   Worsening with examples:    {worsening_count:2d} texts ({worsening_count/total_texts*100:5.1f}%)\")\n",
    "print(f\"   Always wrong:              {always_wrong_count:2d} texts ({always_wrong_count/total_texts*100:5.1f}%)\")\n",
    "print(f\"   Always right:              {always_right_count:2d} texts ({always_right_count/total_texts*100:5.1f}%)\")\n",
    "\n",
    "# Emotion-specific pattern analysis\n",
    "print(f\"\\n EMOTION-SPECIFIC PATTERN ANALYSIS:\")\n",
    "for emotion in emotions:\n",
    "    emotion_df = comparison_df[comparison_df['true_emotion'] == emotion]\n",
    "    emotion_improving = len(emotion_df[\n",
    "        (emotion_df['predicted_emotion_zero'] != emotion) &\n",
    "        (emotion_df['predicted_five'] == emotion)\n",
    "    ])\n",
    "    emotion_always_wrong = len(emotion_df[\n",
    "        (emotion_df['predicted_emotion_zero'] != emotion) &\n",
    "        (emotion_df['predicted_emotion_one'] != emotion) &\n",
    "        (emotion_df['predicted_five'] != emotion)\n",
    "    ])\n",
    "    \n",
    "    print(f\"   {emotion:7}: {emotion_improving:2d} improved, {emotion_always_wrong:2d} always wrong\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS AND CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "MAJOR FINDINGS FROM CASE STUDY:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "1. FEW-SHOT LEARNING EFFECTIVENESS:\n",
    "   â€¢ 31.3% of texts showed improvement with more examples\n",
    "   â€¢ Only 6.3% of texts performed worse with more examples\n",
    "   â€¢ Demonstrates clear net benefit of few-shot learning\n",
    "\n",
    "2. PATTERNS IN IMPROVING TEXTS:\n",
    "   â€¢ Ambiguous emotional expressions benefit most from examples\n",
    "   â€¢ Sarcastic and ironic content shows significant improvement\n",
    "   â€¢ Cultural references and slang become better understood\n",
    "   â€¢ Complex emotional states are clarified with contextual examples\n",
    "\n",
    "3. PATTERNS IN WORSENING TEXTS:\n",
    "   â€¢ Very short texts vulnerable to example bias\n",
    "   â€¢ Some sarcastic content may be over-corrected\n",
    "   â€¢ Rare emotional expressions might not be well-represented in examples\n",
    "\n",
    "4. PERSISTENT CHALLENGES (Always Wrong):\n",
    "   â€¢ Mixed emotions (#sad #happy in same text)\n",
    "   â€¢ Heavy sarcasm with contradictory indicators\n",
    "   â€¢ Cultural references outside training distribution\n",
    "   â€¢ Extremely short texts with minimal context\n",
    "\n",
    "5. CONSISTENT SUCCESSES (Always Right):\n",
    "   â€¢ Clear, unambiguous emotional vocabulary\n",
    "   â€¢ Consistent use of emotion-specific emojis\n",
    "   â€¢ Straightforward emotional expressions\n",
    "   â€¢ Well-established emotional patterns\n",
    "\n",
    "PRACTICAL IMPLICATIONS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "âœ“ STRATEGIC EXAMPLE SELECTION: Choose diverse, representative examples\n",
    "âœ“ DOMAIN ADAPTATION: Tailor examples to specific use cases\n",
    "âœ“ ENSEMBLE APPROACHES: Combine few-shot with other techniques\n",
    "âœ“ CONFIDENCE SCORING: Identify texts likely to need human review\n",
    "\n",
    "FUTURE DIRECTIONS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "â€¢ Multi-label classification for mixed emotions\n",
    "â€¢ Context-aware emotion recognition\n",
    "â€¢ Cross-cultural emotion understanding\n",
    "â€¢ Real-time adaptation of few-shot examples\n",
    "\n",
    "CONCLUSION:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "The case study reveals that few-shot learning provides substantial benefits\n",
    "for emotion classification, with significant improvement in challenging cases\n",
    "outweighing the minor deterioration in a small subset of texts. The patterns\n",
    "identified highlight both the power and limitations of current approaches,\n",
    "pointing toward promising directions for future research and application\n",
    "development in emotion AI.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='toc1_5_11_'></a>[**2.7 Extra LLM Related Materials:**](#toc0_)\n",
    "So this will be it for the lab, but here are some extra materials if you would like to explore:\n",
    "\n",
    "- **How to use OpenAI ChatGPT model's API (Not Free API):** [Basics Video](https://www.youtube.com/watch?v=e9P7FLi5Zy8), [Basics GitHub](https://github.com/gkamradt/langchain-tutorials/blob/main/chatapi/ChatAPI%20%2B%20LangChain%20Basics.ipynb), [RAG's Basics Video](https://www.youtube.com/watch?v=9AXP7tCI9PI&t=300s), [RAG's Basics GitHub](https://github.com/techleadhd/chatgpt-retrieval)\n",
    "\n",
    "- **Advanced topic - QLoRA (Quantized Low-Rank Adapter):** QLoRA is a method used to make fine-tuning large language models more efficient. It works by adding a small, trainable part (LoRA) to a pre-trained model, while keeping the rest of the model frozen. At the same time, it reduces the size of the modelâ€™s data using a process called quantization, which makes the model require less memory. This allows you to fine-tune large models without needing as much computational power, making it easier to adapt models for specific tasks. Materials: [Paper GitHub](https://github.com/artidoro/qlora?tab=readme-ov-file), [Llama 3 Application Video](https://www.youtube.com/watch?v=YJNbgusTSF0&t=512s),[Llama 3 Application GitHub](https://github.com/adidror005/youtube-videos/blob/main/LLAMA_3_Fine_Tuning_for_Sequence_Classification_Actual_Video.ipynb)\n",
    "\n",
    "- **How to Fine-tune and run local LLMs with the `unsloth` library:** [unsloth tutorials](https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms)\n",
    "\n",
    "- **Google's Agent Development Kit Documentation:** [ADK](https://google.github.io/adk-docs/)\n",
    "\n",
    "- **Build AI agents with LangGraph:** [LangGraph Documentation](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fF1woa8YTp5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4e5eiVLOYTp5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 594.85,
   "position": {
    "height": "40px",
    "left": "723px",
    "right": "20px",
    "top": "80px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
